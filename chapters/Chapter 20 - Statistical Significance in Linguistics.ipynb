{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 20 - Statistical Significance in Linguistics\n",
    "\n",
    "In this chapter, I will provide a very basic overview of statistical significance\n",
    "tests for linguistic entities. Continuing from Chapter 19, we've defined a linguistic\n",
    "hypothesis as the following:\n",
    "\n",
    "    a \"statement postulating relationships between\n",
    "    [two or more] constructs\". (Stefanowitsch 2020: 64)\n",
    "    \n",
    "We also pointed out that the primary means of testing linguistic hypotheses is\n",
    "with statistical methods. \n",
    "\n",
    "Statistics possesses a powerful toolkit for testing hypotheses regarding relationships\n",
    "between two categories. These tools are necessary for determining, for instance, how\n",
    "effective a given medicine is on a test population. In medical trials, there is typically\n",
    "two groups: one which receives the drug being tested, and a control group which receives\n",
    "a placebo drug. Data is gathered on how each group responds to certain conditions, such as \n",
    "whether the drug improves a person's symptoms. If the group who receives the drug shows\n",
    "improvement above a certain threshold, statistical significance, then the study can reject\n",
    "the null hypothesis:\n",
    "\n",
    "    the null hypothesis is that there is no significant \n",
    "    relationship between two or more categories\n",
    "    \n",
    "Thus, if the trial group shows improvement above a threshold of statistical significance,\n",
    "the researchers can reject the null hypothesis. In other words, they cannot disprove \n",
    "that there *is* an effect caused by the drug. \n",
    "\n",
    "**Important: in science we aim to falsify claims, not to prove them**. This is key. In the\n",
    "drug study, it would be irresponsible for the researchers to immediately claim their results\n",
    "prove a drug's effectiveness before their results have been replicated by others. That is because\n",
    "there are always other potential factors involved, and the study involves only a subset of an\n",
    "entire population. If a sufficient number of other researchers cannot falsify the results with\n",
    "other similar trials, then the scientific community can tentatively adopt the hypothesis\n",
    "that there is a positive effect caused by the drug.\n",
    "\n",
    "**Important: correlation is not causation**. This phrase is nearly a cliché by now. But it is\n",
    "especially important to keep in mind when working with linguistic data. Language is a very \n",
    "interconnected and complex network of patterns and meanings that interact in all sorts of ways.\n",
    "You should always take care when formulating claims based on statistical data.\n",
    "\n",
    "**Important: statistical data is always *interpreted***. Statistical results must, in the end,\n",
    "make sense of your dataset. I like how Stefanowitsch puts it, regarding the difference between\n",
    "intuitional data (such as grammaticality judgments) and interpretive data (such as reading\n",
    "statistical results): \n",
    "\n",
    "> We need to distinguish two different kinds of introspection: \n",
    "> (i) *intuiting*, i.e. the practice of introspectively accessing \n",
    "> one's linguistic experience in order to create sentences and assign\n",
    "> grammaticality judgments to them and (ii) *interpreting*, i.e. the\n",
    "> practice of assigning an interpretation (in semantic and pragmatic terms)\n",
    "> to and utterance. (Stefanowitsch, *Corpus Linguistics*, 2020, 8).\n",
    "\n",
    "Indeed, we can extend this reasoning to the interpretation of statistical data. Statistical methods\n",
    "are not simply a kind of machine that produce *results*. Even unsupervised methods\n",
    "do not produce results on their own. True results are interpreted against the backdrop\n",
    "of data. However, even though we rely on our subjective interpretation, we are in a \n",
    "better position to be guided by the data than by creating our own data to analyze (sort of\n",
    "like grading your own homework).\n",
    "\n",
    "**Important: these chapters are not a substitute for a proper introduction to statistics.** No really.\n",
    "There are a lot of important concepts that I will not cover here for the sake of time and space. \n",
    "The concept of normal distributions, for example, is extremely important for statistics. But since\n",
    "much language data is not normally distributed, we will not discuss that here. Nevertheless, you\n",
    "should consult real statistical guides before applying these own methods to your own research. For this,\n",
    "I highly recommend either Stefanowitsch, Gries, or Levshina:\n",
    "\n",
    "* [Anatol Stafanowitsch, *Corpus Linguistics: A Guide to Methodology*, 2020.](https://langsci-press.org/catalog/book/148)\n",
    "* [Stefan Gries, *Statistics for Linguistics with R*, 2013.](https://www.degruyter.com/view/title/300775)\n",
    "* [Natalia Levshina, *How to do Linguistics with  R: Data Exploration and Statistical Analysis*, 2015.](https://benjamins.com/catalog/z.195)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are raw counts not sufficient?\n",
    "\n",
    "Please [read this article](https://www.researchgate.net/publication/290329368_Quantitative_designs_and_statistical_techniques) \n",
    "to see why you should avoid using only raw counts in your linguistic research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Dataset from the Hebrew Bible\n",
    "\n",
    "Below we construct a new dataset that we'll refer to throughout this chapter. \n",
    "That dataset is constructed using the [corpus analysis tool Text-Fabric](https://annotation.github.io/text-fabric/)\n",
    "using the linguistic annotations provided by the [ETCBC](http://github.com/etcbc/bhsa). If you don't have Text-Fabric\n",
    "installed already, go ahead and run the first cell.\n",
    "\n",
    "Here is our hypothesis that we will test:\n",
    "\n",
    "    There is a relationship between discourse\n",
    "    type in the Hebrew Bible and word order in\n",
    "    the clause\n",
    "    \n",
    "Here is the operationalizations we make to test this hypothesis:\n",
    "\n",
    "* discourse type - \"N\" for narrative, \"Q\" for quotation\n",
    "* word order - \"SV\" for subject before verb, \"VS\" for verb before subject\n",
    "* clause - a clause as delineated in the BHSA by the ETCBC Amsterdam; only clauses with a subject and verb\n",
    "* Hebrew Bible - BHS linguistically encoded by the ETCBC Amsterdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have Text-Fabric installed,\n",
    "# uncomment below and run\n",
    "\n",
    "#! pip install text-fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules for analysis\n",
    "import pandas as pd\n",
    "import pprint # for pretty printing dictionaries\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the BHSA dataset\n",
    "from tf.app import use\n",
    "\n",
    "bhsa = use('bhsa') # downloads and loads the HB data — may take some time to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore for the moment the messages about the rate limits.\n",
    "\n",
    "Our data has loaded. When working with Text-Fabric, there are a set of classes and methods that we \n",
    "regularly use to navigate a corpus. I assign those classes/methods now to a \n",
    "set of short-form variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = bhsa.api.F # for features of nodes\n",
    "L = bhsa.api.L # for moving through contexts\n",
    "T = bhsa.api.T # for accessing convenient text/reference data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these two variables we can interate through the whole BHSA dataset\n",
    "and collect our data. We will place it into a **raw data table** as is\n",
    "standard.\n",
    "\n",
    "We iterate through every clause in the Hebrew Bible. If the clause has \n",
    "a subject and a verb, we store that clause along with the relevant data.\n",
    "We include reference data so we can locate it later if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhsa_raw_data = []\n",
    "\n",
    "# iterate through the necessary items in BHSA\n",
    "# in Text-Fabric, each linguistic unit is converted\n",
    "# to an integer that can be used to look up relevant\n",
    "# linguistic/contextual data. We use `F` and `L` to \n",
    "# do that. `F` looks up features, whereas `L` looks\n",
    "# up other units within a supplied context\n",
    "\n",
    "for clause in F.otype.s('clause'): # iterate through every clause in HB\n",
    "    \n",
    "    word_order = '' # store order info here\n",
    "    phrases = {} # store phrase nodes here\n",
    "\n",
    "    # word order string is added to as \n",
    "    # constituents are encountered; the \n",
    "    # constituents themselves are saved in\n",
    "    # `phrases` dict for later reference\n",
    "    for phrase in L.d(clause,'phrase'):\n",
    "        function = F.function.v(phrase)\n",
    "        if function == 'Subj':\n",
    "            word_order += 'S'\n",
    "            phrases['S'] = phrase\n",
    "        elif function == 'Pred':\n",
    "            word_order += 'V'\n",
    "            phrases['V'] = phrase\n",
    "            \n",
    "    # retrieve the discourse type of the clause\n",
    "    discourse_type = F.txt.v(clause)\n",
    "            \n",
    "    # now we make a bunch of qualifications about \n",
    "    # which clauses we should keep and which we should skip\n",
    "        \n",
    "    # skip clause if there is no subject or verb \n",
    "    # we'll use sets to check for both values\n",
    "    # if there is anything lacking from set {'S','V'}\n",
    "    # then the clause does not qualify!\n",
    "    if {'S','V'} - set(phrases.keys()):\n",
    "        continue\n",
    "    \n",
    "    # skip if too many phrases matched (i.e. multiple constituents)\n",
    "    if len(phrases) > 2:\n",
    "        continue\n",
    "        \n",
    "    # skip clause if the discourse type is not N or Q\n",
    "    if discourse_type not in {'N', 'Q'}:\n",
    "        continue\n",
    "        \n",
    "    # assemble some English glosses for phrases\n",
    "    kind2gloss = {}\n",
    "    for kind,phrase in phrases.items():\n",
    "        glosses = []\n",
    "        for word in L.d(phrase,'word'):\n",
    "            glosses.append(F.gloss.v(word))    \n",
    "        glossed = ' '.join(glosses)\n",
    "        kind2gloss[kind] = glossed\n",
    "        \n",
    "    # now we assemble the rest of the data we want to store\n",
    "    book,chapter,verse = T.sectionFromNode(clause)\n",
    "    reference = f'{book} {chapter}:{verse}'\n",
    "    subj_phrase_gloss = kind2gloss['S']\n",
    "    verb_phrase_gloss = kind2gloss['V']\n",
    "    clause_text = T.text(clause)\n",
    "    \n",
    "    # build up clause data as a big dict\n",
    "    clause_data = {\n",
    "        'reference': reference,\n",
    "        'book': book,\n",
    "        'clause_node': clause,\n",
    "        'text': clause_text,\n",
    "        'word_order': word_order,\n",
    "        'discourse_type': discourse_type,\n",
    "        'subj_phrase': phrases['S'],\n",
    "        'verb_phrase': phrases['V'],\n",
    "        'subj_glossed': kind2gloss['S'],\n",
    "        'verb_glossed': kind2gloss['V'],\n",
    "    }\n",
    "    \n",
    "    # add data to whole dataset\n",
    "    bhsa_raw_data.append(clause_data)\n",
    "    \n",
    "# report on our data\n",
    "print(len(bhsa_raw_data), 'clauses gathered')\n",
    "print('sample:')\n",
    "pprint.pprint(bhsa_raw_data[:3], sort_dicts=False) # pprint will print dicts with nice indentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we arrange the data into a DataFrame for analysis. Note that since we used\n",
    "a dictionary for each row, we do not have to specify column labels. They are inferred\n",
    "from the data by Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_df = pd.DataFrame(bhsa_raw_data)\n",
    "\n",
    "clause_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before you begin to analyze a dataset, you should always start by exploring your dataset.\n",
    "This ensures that your data is arranged as expected.\n",
    "\n",
    "A particularly useful method with a DataFrames is `value_counts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_df['word_order'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_df['discourse_type'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a lot more of `VS` and `N` types than `SV` and `Q`. Remember our hypothesis:\n",
    "\n",
    "    There is a relationship between discourse\n",
    "    type in the Hebrew Bible and word order in\n",
    "    the clause\n",
    "\n",
    "This immediately raises the question: How do we ensure that our comparisons will \n",
    "be \"fair\"? Since `VS` and `N` occur so much more frequently, we might find it \n",
    "difficult to reach a sound conclusion. If `VS` occurs so much more in `N`, it \n",
    "may not be because there is a meaningful relationship there, it could just be \n",
    "due to sample sizes.\n",
    "\n",
    "Let's try to look at proportions first. We will need a summary table of how often\n",
    "each of these respective values co-occur. We will use the Pandas method called\n",
    "the [pivot table](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html) \n",
    "to cross-tabulate the counts of the values together.\n",
    "\n",
    "Note that because we are interested in whether discourse type affects word order,\n",
    "discourse type is our independent variable. To put it another way, we hypothesize\n",
    "that word order is dependent on discourse type. \n",
    "\n",
    "**Remember that we put independent variables in rows (index in DataFrame) and \n",
    "dependent variables in columns**. You will see these values reflected in the\n",
    "`pivot_table` below. The method (`aggfunc`) we use for counting occurrences is called\n",
    "`size`. It is one of several count methods you can supply to a pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse_order = pd.pivot_table(clause_df, index='discourse_type', columns='word_order', aggfunc='size')\n",
    "\n",
    "discourse_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the raw co-occurrence frequencies for these values, calculated directly\n",
    "from our raw data. We can convert this to a proportion across discourse type. That is,\n",
    "for each discourse type, we calculate what percentage is represented by SV or VS.\n",
    "\n",
    "We can use Pandas `.divide` and `.sum` to specify how to add and divide each row/column.\n",
    "In Pandas a `0` is the \"index\" and a `1` is a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse_proportion = discourse_order.divide(discourse_order.sum(1), 0)\n",
    "\n",
    "discourse_proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know we've done it correctly because the sum of N and Q add up to 1 (i.e. 100%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse_proportion.sum(1) # sum across columns (i.e. for each row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 10% of narrative discourse occur with SV word order, for instance.\n",
    "\n",
    "But notice what we've done: we are looking at what proportion of discourse type is\n",
    "represented by a certain word order. We can also look at that from the other perspective:\n",
    "what proportion of a certain word order is represented by a discourse type? In other words,\n",
    "there is a bi-directional nature of proportionality here, represented across all 4 possible\n",
    "combinations.\n",
    "\n",
    "Below, we calculate the proportions across word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_proportion = discourse_order.divide(discourse_order.sum(0), 1)\n",
    "\n",
    "order_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_proportion.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that a higher proportion of SV occurs with N than does N with SV!** In the discourse proportions\n",
    "we saw that SV accounts for 10% of all of N's occurrences. But here we see that\n",
    "N accounts for 35% of SV's occurrences. That's a substantial gap!\n",
    "\n",
    "This is why we need **contingency data**. Contingency data takes into account\n",
    "all values. Those values can then be used to calculate various measures of \n",
    "statistical significance.\n",
    "\n",
    "The statistical significance will then allow us to answer our question about the relationship\n",
    "between word order and discourse type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The contingency table\n",
    "\n",
    "The typical form of a contingency table for language data\n",
    "is the following (see Levshina 224 for more):\n",
    "\n",
    "|         | feature | ¬feature |\n",
    "|---------|---------|----------|\n",
    "| sample  |    A    |     B    |\n",
    "| ¬sample |    C    |     D    |\n",
    "\n",
    "Where sample is a independent entity, feature is the dependent\n",
    "entity, and where ¬ means \"not\", i.e. ¬feature means every other\n",
    "feature besides the first feature. \n",
    "\n",
    "The values A, B, C, D are sums of those respective cross tabulations.\n",
    "So A is the sum of the co-occurrence of sample x feature. B is the sum\n",
    "of a sample x all other features, etc.\n",
    "\n",
    "Given a sample and a feature count in a dataset, the math \n",
    "for finding A, B, C, D (see Levshina 2015, 223) is:\n",
    "\n",
    "```\n",
    "        A = frequency of sample w/ feature (in dataset)\n",
    "        B = sum(sample) - A\n",
    "        C = sum(feature) - A\n",
    "        D = sum(dataset) - (A+B+C)\n",
    "```\n",
    "\n",
    "What this means for our dataset above is that each one of the 4 values \n",
    "[i.e. (N,D) x (SV,VS)] has its own contingency data. That means we need\n",
    "to calculate the values independently for each one and assemble them into a new\n",
    "form.\n",
    "\n",
    "There are more efficient and less efficient ways to do this. But the simplest\n",
    "way, when dealing with a relatively small dataset, is to do a double for loop. \n",
    "We do that below and re-assemble the data into 4 different tables each corresponding\n",
    "to a, b, c, and d and store them in a dict. Then, the values of each of the 4 \n",
    "tables will line up with the original sample x feature pairs. If this is confusing,\n",
    "try to follow the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_tables(counts_df):\n",
    "    \"\"\"Calculate contingency tables for a DF of raw counts\"\"\"\n",
    "    \n",
    "    # each table assembled here\n",
    "    # the list will become a list of row lists\n",
    "    cont_values = {\n",
    "        'a': [],\n",
    "        'b': [],\n",
    "        'c': [],\n",
    "        'd': [],\n",
    "    }\n",
    "\n",
    "    # iterate through raw data table and \n",
    "    # assemble a,b,c,d matrices\n",
    "    for sample in discourse_order.index:\n",
    "\n",
    "        # new rows of a,b,c,d assembled here\n",
    "        a_row = []\n",
    "        b_row = []\n",
    "        c_row = []\n",
    "        d_row = []\n",
    "\n",
    "        # calculate the contigency scores \n",
    "        for feature in discourse_order.columns:\n",
    "            a = counts_df.loc[sample][feature]\n",
    "            b = counts_df.loc[sample].sum() - a\n",
    "            c = counts_df[feature].sum() - a\n",
    "            d = counts_df.sum().sum() - (a+b+c) # .sum().sum() to get whole dataset sum\n",
    "\n",
    "            # stash in new rows\n",
    "            a_row.append(a) \n",
    "            b_row.append(b)\n",
    "            c_row.append(c)\n",
    "            d_row.append(d)\n",
    "\n",
    "        # add new row lists to table list\n",
    "        cont_values['a'].append(a_row)\n",
    "        cont_values['b'].append(b_row)\n",
    "        cont_values['c'].append(c_row)\n",
    "        cont_values['d'].append(d_row)\n",
    "  \n",
    "    # return the dictionary\n",
    "    return cont_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the new function to our dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_values = contingency_tables(discourse_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(cont_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dictionary value can be converted back to a DataFrame so that we can work with it. We do that below.\n",
    "Note that we must re-specify the name of the indices and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value, table in cont_values.items():\n",
    "    cont_values[value] = pd.DataFrame(table, index=discourse_order.index, columns=discourse_order.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can refer to each dataframe directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_values['a'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a is the same as a raw dataframe. That is expected. But look at b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_values['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of succinctness, we assign each value now to its own variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some short-hand names for clearer code\n",
    "a = cont_values['a']\n",
    "b = cont_values['b']\n",
    "c = cont_values['c']\n",
    "d = cont_values['d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Association with Fisher's Exact\n",
    "\n",
    "The Fisher's Exact test is a powerful test for statistical significance. Fisher's returns\n",
    "a value, called the `p-value`, which is an important measure in statistics of significance.\n",
    "\n",
    "**The p-value tells us the likelihood that two variables are \n",
    "would co-occur with a given frequency by chance.** Thus, the lower \n",
    "the p-value, the more likely there is a significant and meaningful\n",
    "relationship. \n",
    "\n",
    "**By convention, a p-value < 0.05 is accepted as statistically significant**. This is \n",
    "simply a standard often observed in statistics (though not always). A p-value < 0.05\n",
    "means that there is a less than 5% chance that the observed frequency is due to random\n",
    "chance. At that threshold, we are safe to reject the null hypothesis, that there is no relationship.\n",
    "\n",
    "Fisher's Exact can be calculated using the `scipy` stats module. We also import numpy as a helper tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the Fisher's test requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(stats.fisher_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the test takes a 2x2 contingency table. \n",
    "\n",
    "We will now build a DataFrame with the Fisher's test. We use feed the test\n",
    "the requested contingency data as we iterate through the dataset. We assemble\n",
    "the new data into a new dataframe arranged similarly to the old dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishers_data = []\n",
    "\n",
    "for sample in discourse_order.index:\n",
    "    new_row = []\n",
    "    for feature in discourse_order.columns:\n",
    "        \n",
    "        # use numpy to form new matrix\n",
    "        contingency = np.matrix(\n",
    "            [\n",
    "                [a[feature][sample], b[feature][sample]], \n",
    "                [c[feature][sample], d[feature][sample]]\n",
    "            ]\n",
    "        )\n",
    "        oddsratio, p_value = stats.fisher_exact(contingency)\n",
    "        new_row.append(p_value)\n",
    "    fishers_data.append(new_row)\n",
    "    \n",
    "# make new DF\n",
    "fishers_data = pd.DataFrame(fishers_data, index=a.index, columns=a.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishers_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the answer is given to us with scientific notation. These numbers\n",
    "are so small, over 266 decimal places, that they are nearly 0. That means that\n",
    "according to the Fisher's test, there is a near zero chance each of these 4\n",
    "values is due to randomness.\n",
    "\n",
    "**Based on the Fisher's Exact analysis, we can reject the null hypothesis that \n",
    "discourse type has no effect on word order in the Hebrew Bible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Association with ΔP\n",
    "\n",
    "We are finally in a position to posit an answer for our research question. For this,\n",
    "we need an appropriate measure of statistical significance. ΔP is such a measure, designed\n",
    "for testing hypotheses in Psycholinguistics. \n",
    "\n",
    "It is worth reading about ΔP separately (see article in the Slack channel), because it is a powerful tool for measuring\n",
    "the relationships of linguistic items. Here's a few points about ΔP:\n",
    "\n",
    "* based on the concept of a cognitive \"cue\" and \"response\", where a \"cue\" is the independent value, and the \"response\" is the dependent value\n",
    "* not based on normal distribution assumptions\n",
    "* returns a value from 1 to -1, where 1 is the strongest association, and -1 is the strongest *dis*association.\n",
    "\n",
    "ΔP can be calculated with the data we've just organized. Here's the formula for it:\n",
    "\n",
    "$\\frac{a}{(a+b)} - \\frac{c}{(c+d)}$\n",
    "\n",
    "Where a, b, c, and d are the contigency values we've already calculated.\n",
    "\n",
    "This represents the probability of an observed collocation\n",
    "(C) given a target construction (CX) minus the probability \n",
    "of the observed collocation without the target construction \n",
    "(adapted from Ellis 2006: 11):\n",
    "\n",
    "$P(C|CX) - P(C|-CX)$\n",
    "\n",
    "If the math is weird to you, just follow the Python code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate DP for our dataset\n",
    "# note we use the parentheses in Python\n",
    "# to tell it order of operations, this prevents\n",
    "# b - c, for example\n",
    "# note also that a, b, c, d are each full-fledged DataFrames!\n",
    "# thus we calculate DP for the whole dataset at once\n",
    "\n",
    "dp_df = a/(a+b) - c/(c+d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at our table. We can read the table thusly:\n",
    "\n",
    "What is probabilty that the item in row, cues or triggers, the item in the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are the final results!** We can read this as: \n",
    "\n",
    "* If we are in narrative, we are 30% less likely to see SV, and 30% more likely to see VS\n",
    "* If we are in quotation, we are 30% more likely to see SV, and 30% less likely to see VS\n",
    "\n",
    "**Based on the ΔP analysis, we can reject the null hypothesis that \n",
    "discourse type has no effect on word order in the Hebrew Bible.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
