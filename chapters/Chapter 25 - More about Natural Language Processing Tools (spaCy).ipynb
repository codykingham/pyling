{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material is copied (possibily with some modifications) from the [Python for Text-Analysis course](https://github.com/cltl/python-for-text-analysis/tree/master/Chapters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 25 - More about Natural Language Processing Tools (spaCy)\n",
    "\n",
    "Text data is unstructured. But if you want to extract information from text, then you often need to process that data into a more structured representation. The common idea for all Natural Language Processing (NLP) tools is that they try to structure or transform text in some meaningful way. You have already learned about four basic NLP steps: sentence splitting, ≈çtokenization, POS-tagging and lemmatization. For all of these, we have used the NLTK library, which is widely used in the field of NLP. However, there are some competitors out there that are worthwhile to have a look at. One of them is spaCy, which is fast and accurate and supports multiple languages. \n",
    "\n",
    "**At the end of this chapter, you will be able to:**\n",
    "- work with spaCy\n",
    "- find some additional NLP tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The NLP pipeline\n",
    "\n",
    "There are many tools and libraries designed to solve NLP problems. In Chapter 15, we have already seen the NLTK library for tokenization, sentence splitting, part-of-speech tagging and lemmatization. However, there are many more NLP tasks and off-the-shelf tools to perform them. These tasks often depend on each other and are therefore put into a sequence; such a sequence of NLP tasks is called an NLP pipeline. Some of the most common NLP tasks are:\n",
    "\n",
    "* **Tokenization:** splitting texts into individual words\n",
    "* **Sentence splitting:** splitting texts into sentences\n",
    "* **Part-of-speech (POS) tagging:** identifying the parts of speech of words in context (verbs, nouns, adjectives, etc.)\n",
    "* **Morphological analysis:** separating words into morphemes and identifying their classes (e.g. tense/aspect of verbs)\n",
    "* **Stemming:** identifying the stems of words in context by removing inflectional/derivational affixes, such as 'troubl' for 'trouble/troubling/troubled'\n",
    "* **Lemmatization:** identifying the lemmas (dictionary forms) of words in context, such as 'go' for 'go/goes/going/went'\n",
    "* **Word Sense Disambiguation (WSD):** assigning the correct meaning to words in context\n",
    "* **Stop words recognition:** identifying commonly used words (such as 'the', 'a(n)', 'in', etc.) in text, possibly to ignore them in other tasks\n",
    "* **Named Entity Recognition (NER):** identifying people, locations, organizations, etc. in text\n",
    "* **Constituency/dependency parsing:** analyzing the grammatical structure of a sentence\n",
    "* **Semantic Role Labeling (SRL):** analyzing the semantic structure of a sentence (*who* does *what* to *whom*, *where* and *when*)\n",
    "* **Sentiment Analysis:** determining whether a text is mostly positive or negative\n",
    "* **Word Vectors (or Word Embeddings) and Semantic Similarity:** representating the meaning of words as rows of real valued numbers where each point captures a dimension of the word's meaning and where semantically similar words have similar vectors (very popular these days)\n",
    "\n",
    "You don't always need all these modules. But it's important to know that they are\n",
    "there, so that you can use them when the need arises.\n",
    "\n",
    "### 1.1 How can you use these modules?\n",
    "\n",
    "Let's be clear about this: **you don't always need to use Python for this**. There are\n",
    "some very strong NLP programs out there that don't rely on Python. You can typically\n",
    "call these programs from the command line. Some examples are:\n",
    "\n",
    "* [Treetagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/) is a POS-tagger\n",
    "  and lemmatizer in one. It provides support for many different languages. If you want to\n",
    "  call Treetagger from Python, use [treetaggerwrapper](http://treetaggerwrapper.readthedocs.io/).\n",
    "  [Treetagger-python](https://github.com/miotto/treetagger-python) also works, but is much slower.\n",
    "\n",
    "* [Stanford's CoreNLP](http://stanfordnlp.github.io/CoreNLP/) is a very powerful system\n",
    "  that is able to process English, German, Spanish, French, Chinese and Arabic. (Each to\n",
    "  a different extent, though. The pipeline for English is most complete.) There are also\n",
    "  Python wrappers available, such as [py-corenlp](https://github.com/smilli/py-corenlp).\n",
    "\n",
    "* [The Maltparser](http://www.maltparser.org/) has models for English, Swedish, French, and Spanish.\n",
    "\n",
    "\n",
    "Having said that, there are many **NLP-tools that have been developed for Python**:\n",
    "\n",
    "* [Natural Language ToolKit (NLTK)](http://www.nltk.org/): Incredibly versatile library with a bit of everything.\n",
    "  The only downside is that it's not the fastest library out there, and it lags behind the\n",
    "  state-of-the-art.\n",
    "    * Access to several corpora.\n",
    "    * Create a POS-tagger. (Some of these are actually state-of-the-art if you have enough training data.)\n",
    "    * Perform corpus analyses.\n",
    "    * Interface with [WordNet](https://wordnet.princeton.edu/).\n",
    "* [Pattern](http://www.clips.ua.ac.be/pattern): A module that describes itself as a 'web mining module'. Implements a\n",
    "    tokenizer, tagger, parser, and sentiment analyzer for multiple different languages.\n",
    "    Also provides an API for Google, Twitter, Wikipedia and Bing.\n",
    "* [Textblob](http://textblob.readthedocs.io/en/dev/): Another general NLP library that builds on the NLTK and Pattern.\n",
    "* [Gensim](http://radimrehurek.com/gensim/): For building vector spaces and topic models.\n",
    "* [Corpkit](http://corpkit.readthedocs.io/en/latest/) is a module for corpus building and corpus management. Includes an interface to the Stanford CoreNLP parser.\n",
    "* [SpaCy](https://spacy.io/): Tokenizer, POS-tagger, parser and named entity recogniser for English, German, Spanish, Portugese, French, Italian and Dutch (more languages in progress). It can also predict similarity using word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. spaCy\n",
    "\n",
    "[spaCy](https://spacy.io/) provides a rather complete NLP pipeline: it takes a raw document and performs tokenization, POS-tagging, stop word recognition, morphological analysis, lemmatization, sentence splitting, dependency parsing and Named Entity Recognition (NER). It also supports similarity prediction, but that is outside of the scope of this notebook. The advantage of SpaCy is that it is really fast, and it has a good accuracy. In addition, it currently supports multiple languages, among which: English, German, Spanish, Portugese, French, Italian and Dutch. \n",
    "\n",
    "In this notebook, we will show you the basic usage. If you want to learn more, please visit spaCy's website; it has extensive documentation and provides excellent user guides. \n",
    "\n",
    "### 2.1 Installing and loading spaCy\n",
    "\n",
    "To install spaCy, check out the instructions [here](https://spacy.io/usage). On this page, it is explained exactly how to install spaCy for your operating system, package manager and desired language model(s). Simply run the suggested commands in your terminal or cmd. Alternatively, you can probably also just run the following cells in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first load spaCy. We import the spaCy module and load the English tokenizer, tagger, parser, NER and word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # other languages: de, es, pt, fr, it, nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nlp` is now a Python object representing the English NLP pipeline that we can use to process a text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRA: Larger models\n",
    "\n",
    "For English, there are three [models](https://spacy.io/usage/models) ranging from 'small' to 'large':\n",
    "\n",
    "- en_core_web_sm\n",
    "- en_core_web_md\n",
    "- en_core_web_lg\n",
    "\n",
    "By default, the smallest one is loaded. Larger models should have a better accuracy, but take longer to load. If you like, you can use them instead. You will first need to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment one of the lines below if you want to load the medium or large model instead of the small one\n",
    "#nlp = spacy.load('en_core_web_md')  \n",
    "nlp = spacy.load('en_core_web_lg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using spaCy\n",
    "\n",
    "Parsing a text with spaCy after loading a language model is as easy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have an awesome cat. It's sitting on the mat that I bought yesterday.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc` is now a Python object of the class `Doc`. It is a container for accessing linguistic annotations and a sequence of `Token` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc, Token and Span objects\n",
    "\n",
    "At this point, there are three important types of objects to remember:\n",
    "\n",
    "* A `Doc` is a sequence of `Token` objects.\n",
    "* A `Token` object represents an individual token ‚Äî i.e. a word, punctuation symbol, whitespace, etc. It has attributes representing linguistic annotations. \n",
    "* A `Span` object is a slice from a `Doc` object and a sequence of `Token` objects.\n",
    "\n",
    "Since `Doc` is a sequence of `Token` objects, we can iterate over all of the tokens in the text as shown below, or select a single token from the sequence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    print(token)\n",
    "print()\n",
    "\n",
    "# Select one single token by index\n",
    "first_token = doc[0]\n",
    "print(\"First token:\", first_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that even though these look like strings, they are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, \"\\t\", type(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `Token` objects have many useful methods and *attributes*, which we can list by using `dir()`. We haven't really talked about attributes during this course, but while methods are operations or activities performed by that object, attributes are 'static' features of the objects. Methods are called using parantheses (as we have seen with `str.upper()`, for instance), while attributes are indicated without parantheses. We will see some examples below.\n",
    "\n",
    "You can find more detailed information about the token methods and attributes in the [documentation](https://spacy.io/api/token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(first_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some of the attributes of the tokens. Can you figure out what they mean? Feel free to try out a few more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print attributes of tokens\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some of the attributes end with an underscore. For example, tokens have both `lemma` and `lemma_` attributes. The `lemma` attribute represents the id of the lemma (integer), while the `lemma_` attribute represents the unicode string representation of the lemma. In practice, you will mostly use the `lemma_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma(), token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use spacy.explain to find out more about certain labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out some more, such as NN, ADP, PRP, VBD, VBP, VBZ, WDT, aux, nsubj, pobj, dobj, npadvmod\n",
    "spacy.explain(\"VBZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `Span` object from the slice doc[start : end]. For instance, doc[2:5] produces a span consisting of tokens 2, 3 and 4. Stepped slices (e.g. doc[start : end : step]) are not supported, as `Span` objects must be contiguous (cannot have gaps). You can use negative indices and open-ended ranges, which have their normal Python semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Span\n",
    "a_slice = doc[2:5]\n",
    "print(a_slice, type(a_slice))\n",
    "\n",
    "# Iterate over Span\n",
    "for token in a_slice:\n",
    "    print(token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text, sentences and noun_chunks\n",
    "\n",
    "If you call the `dir()` function on a `Doc` object, you will see that it has a range of methods and attributes. You can read more about them in the [documentation](https://spacy.io/api/doc). Below, we highlight three of them: `text`, `sents` and `noun_chunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, `text` simply gives you the whole document as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.text)\n",
    "print(type(doc.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sents` can be used to get all the sentences. Notice that it will create a so-called 'generator'. For now, you don't have to understand exactly what a generator is (if you like, you can read more about them online). Just remember that we can use generators to iterate over an object in a fast and efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the sentences as a generator \n",
    "print(doc.sents, type(doc.sents))\n",
    "\n",
    "# We can use the generator to loop over the sentences; each sentence is a span of tokens\n",
    "for sentence in doc.sents:\n",
    "    print(sentence, type(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find this difficult to comprehend, you can also simply convert it to a list and then loop over the list. Remember that this is less efficient, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also store the sentences in a list and then loop over the list \n",
    "sentences = list(doc.sents)\n",
    "for sentence in sentences:\n",
    "    print(sentence, type(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of converting it to a list is that we can use indices to select certain sentences. For example, in the following we only print some information about the tokens in the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the tokens in the second sentence.\n",
    "sentences = list(doc.sents)\n",
    "for token in sentences[1]:\n",
    "    data = '\\t'.join([token.orth_,\n",
    "                      token.lemma_,\n",
    "                      token.pos_,\n",
    "                      token.tag_,\n",
    "                      str(token.i),    # Turn index into string\n",
    "                      str(token.idx)]) # Turn index into string\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, `noun_chunks` can be used to create a generator for all noun chunks in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the noun chunks as a generator \n",
    "print(doc.noun_chunks, type(doc.noun_chunks))\n",
    "\n",
    "# You can loop over a generator; each noun chunk is a span of tokens\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk, type(chunk))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entities\n",
    "\n",
    "Finally, we can also very easily access the Named Entities in a text using `ents`. As you can see below, it will create a tuple of the entities recognized in the text. Each entity is again a span of tokens, and you can access the type of the entity with the `label_` attribute of `Span`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a slightly longer text, from the Wikipedia page about Harry Potter.\n",
    "harry_potter = \"Harry Potter is a series of fantasy novels written by British author J. K. Rowling.\\\n",
    "The novels chronicle the life of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley,\\\n",
    "all of whom are students at Hogwarts School of Witchcraft and Wizardry.\\\n",
    "The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal,\\\n",
    "overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles.\"\n",
    "\n",
    "doc = nlp(harry_potter)\n",
    "print(doc.ents)\n",
    "print(type(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entity is a span of tokens and is labeled with the type of entity\n",
    "for entity in doc.ents:\n",
    "    print(entity, \"\\t\", entity.label_, \"\\t\", type(entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, but what does NORP mean? Again, you can use spacy.explain() to find out:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EXTRA: Stanford CoreNLP\n",
    "\n",
    "Another very popular NLP pipeline is [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/index.html). You can use the tool from the command line, but there are also some useful Python wrappers that make use of the Stanford CoreNLP API, such as pycorenlp. As you might want to use this in the future, we will provide you with a quick start guide. To use the code below, you will have to do the following:\n",
    "\n",
    "1. Download Stanford CoreNLP [here](https://stanfordnlp.github.io/CoreNLP/download.html).\n",
    "2. Install pycorenlp (run `pip install pycorenlp` in your terminal, or simply run the cell below).\n",
    "3. Open a terminal and run the following commands (replace with the correct directory names):  \n",
    "   `cd LOCATION_OF_CORENLP/stanford-corenlp-full-2018-02-27`  \n",
    "   `java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer`  \n",
    "   This step you will always have to do if you want to use the Stanford CoreNLP API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#pip install pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will want to define which [annotators](https://stanfordnlp.github.io/CoreNLP/annotators.html) to use and which [output format](https://stanfordnlp.github.io/CoreNLP/cmdline.html#output-options) should be produced (text, json, xml, conll, conllu, serialized). Annotating the document then is very easy. Note that Stanford CoreNLP uses some large models that can take [a long time](https://stackoverflow.com/questions/11219392/stanford-corenlp-very-slow) to load. You can read more about it [here](https://stanfordnlp.github.io/CoreNLP/memory-time.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harry_potter = \"Harry Potter is a series of fantasy novels written by British author J. K. Rowling.\\\n",
    "The novels chronicle the life of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley,\\\n",
    "all of whom are students at Hogwarts School of Witchcraft and Wizardry.\\\n",
    "The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal,\\\n",
    "overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles.\"\n",
    "\n",
    "# Define annotators and output format\n",
    "properties= {'annotators': 'tokenize, ssplit, pos, lemma, parse',\n",
    "             'outputFormat': 'json'}\n",
    "\n",
    "# Annotate the string with CoreNLP\n",
    "doc = nlp.annotate(harry_potter, properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we will simply show some examples of how to access the linguistic annotations if you use the properties as shown above. If you'd like to continue working with Stanford CoreNLP in the future, you will likely have to experiment a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = doc[\"sentences\"]\n",
    "first_sentence = sentences[0]\n",
    "first_sentence.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence[\"parse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence[\"basicDependencies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc[\"sentences\"]:\n",
    "    for token in sent[\"tokens\"]:\n",
    "        word = token[\"word\"]\n",
    "        lemma = token[\"lemma\"]\n",
    "        pos = token[\"pos\"]\n",
    "        print(word, lemma, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out what the entity label 'NORP' means\n",
    "spacy.explain(\"NORP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NLTK vs. spaCy vs. CoreNLP\n",
    "\n",
    "There might be different reasons why you want to use NLTK, spaCy or Stanford CoreNLP. There are differences in efficiency, quality, user friendliness, functionalities, output formats, etc. At this moment, we advise you to go with spaCy because of its ease in use and high quality performance.\n",
    "\n",
    "Here's an example of both NLTK and spaCy in action. \n",
    "\n",
    "* The example text is a case in point. What goes wrong here?\n",
    "* Try experimenting with the text to see what the differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I like cheese very much\"\n",
    "\n",
    "print(\"NLTK results:\")\n",
    "nltk_tagged = nltk.pos_tag(text.split())\n",
    "print(nltk_tagged)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"spaCy results:\")\n",
    "doc = nlp(text)\n",
    "spacy_tagged = []\n",
    "for token in doc:\n",
    "    tag_data = (token.orth_, token.tag_,)\n",
    "    spacy_tagged.append(tag_data)\n",
    "print(spacy_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to learn more about the differences between NLTK, spaCy and CoreNLP? Here are some links:\n",
    "- [Facts & Figures (spaCy)](https://spacy.io/usage/facts-figures)\n",
    "- [About speed (CoreNLP vs. spaCy)](https://nlp.stanford.edu/software/tokenizer.html#Speed)\n",
    "- [NLTK vs. spaCy: Natural Language Processing in Python](https://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/) \n",
    "- [What are the advantages of Spacy vs NLTK?](https://www.quora.com/What-are-the-advantages-of-Spacy-vs-NLTK) \n",
    "- [5 Heroic Python NLP Libraries](https://elitedatascience.com/python-nlp-libraries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Some other useful modules for cleaning and preprocessing\n",
    "\n",
    "Data is often messy, noisy or includes irrelevant information. Therefore, chances are big that you will need to do some cleaning before you can start with your analysis. This is especially true for social media texts, such as tweets, chats, and emails. Typically, these texts are informal and notoriously noisy. Normalising them to be able to process them with NLP tools is a NLP challenge in itself and fully discussing it goes beyond the scope of this course. However, you may find the following modules useful in your project:\n",
    "\n",
    "- [tweet-preprocessor](https://pypi.python.org/pypi/tweet-preprocessor/0.4.0): This library makes it easy to clean, parse or tokenize the tweets. It supports cleaning, tokenizing and parsing of URLs, hashtags, reserved words, mentions, emojis and smileys.\n",
    "- [emot](https://pypi.python.org/pypi/emot/1.0): Emot is a python library to extract the emojis and emoticons from a text (string). All the emojis and emoticons are taken from a reliable source, i.e. Wikipedia.org.\n",
    "- [autocorrect](https://pypi.python.org/pypi/autocorrect/0.1.0): Spelling corrector (Python 3).\n",
    "- [html](https://docs.python.org/3/library/html.html#module-html): Can be used to remove HTML tags.\n",
    "- [chardet](https://pypi.python.org/pypi/chardet): Universal encoding detector for Python 2 and 3.\n",
    "- [ftfy](https://pypi.python.org/pypi/ftfy): Fixes broken unicode strings.\n",
    "\n",
    "If you are interested in reading more about these topic, these papers discuss preprocessing and normalization:\n",
    "\n",
    "* [Assessing the Consequences of Text Preprocessing Decisions](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2849145) (Denny & Spirling 2016). This paper is a bit long, but it provides a nice discussion of common preprocessing steps and their potential effects.\n",
    "* [What to do about bad language on the internet](http://www.cc.gatech.edu/~jeisenst/papers/naacl2013-badlanguage.pdf) (Eisenstein 2013). This is a quick read that we recommend everyone to at least look through.\n",
    "\n",
    "And [here](https://www.kaggle.com/rtatman/character-encodings-tips-tricks/) is a nice blog about character encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "1. What is the difference between token.pos\\_ and token.tag\\_? Read [the docs](https://spacy.io/api/annotation#pos-tagging) to find out.\n",
    "\n",
    "2. What do the different labels mean? Use `space.explain` to inspect some of them. You can also refer to [this page](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) for a complete overview. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have an awesome cat. It's sitting on the mat that I bought yesterday.\")\n",
    "for token in doc:\n",
    "    print(token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"PRON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Let's practice a bit with processing files. Open the file `charlie.txt` for reading and use `read()` to read its content as a string. Then use spaCy to annotate this string and print the information below. Remember: you can use `dir()` to remind yourself of the attributes.\n",
    "\n",
    "For each **token** in the text:\n",
    "1. Text \n",
    "2. Lemma\n",
    "3. POS tag\n",
    "4. Whether it's a stopword or not\n",
    "5. Whether it's a punctuation mark or not\n",
    "\n",
    "For each **sentence** in the text:\n",
    "1. The complete text\n",
    "2. The number of tokens\n",
    "3. The complete text in lowercase letters\n",
    "4. The text, lemma and POS of the first word\n",
    "\n",
    "For each **noun chunk** in the text:\n",
    "1. The complete text\n",
    "2. The number of tokens\n",
    "3. The complete text in lowercase letters\n",
    "4. The text, lemma and POS of the first word\n",
    "\n",
    "For each **named entity** in the text:\n",
    "1. The complete text\n",
    "2. The number of tokens\n",
    "3. The complete text in lowercase letters\n",
    "4. The text, lemma and POS of the first word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/charlie.txt\"\n",
    "\n",
    "# read the file and process with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all information about the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all information about the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all information about the noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all information about the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Remember how we can use the `os` and `glob` modules to process multiple files? For example, we can read all `.txt` files in the `dreams` folder like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filenames = glob.glob(\"../data/Dreams/*.txt\")\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a function called `get_vocabulary` that takes one positional parameter `filenames`. It should read in all `filenames` and return a set called `unique_words`, that contains all unique words in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(filenames):\n",
    "    # your code here\n",
    "\n",
    "# test your function here\n",
    "unique_words = get_vocabulary(filenames)\n",
    "print(unique_words, len(unique_words))\n",
    "assert len(unique_words) == 415 # if your code is correct, this should not raise an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "Create a function called `get_sentences_with_keyword` that takes one positional parameter `filenames` and one keyword parameter `filenames` with default value `None`. It should read in all `filenames` and return a list called `sentences` that contains all sentences (the complete texts) with the keyword. \n",
    "\n",
    "Hints:\n",
    "- It's best to check for the *lemmas* of each token\n",
    "- Lowercase both your keyword and the lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filenames = glob.glob(\"../data/Dreams/*.txt\")\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_keyword(filenames, keyword=None):\n",
    "    #your code here\n",
    "\n",
    "# test your function here\n",
    "sentences = get_sentences_with_keyword(filenames, keyword=\"toy\")\n",
    "print(sentences)\n",
    "assert len(sentences) == 4 # if your code is correct, this should not raise an error"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
