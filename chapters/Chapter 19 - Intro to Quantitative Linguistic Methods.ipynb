{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19 - Introduction to Quantitative Linguistic Methods\n",
    "\n",
    "Quantitative linguistics covers a huge variety of subfields and individual methods,\n",
    "spanning not only linguistics itself, but computer science, statistics, information\n",
    "theory, and more.\n",
    "\n",
    "We can generally make a distinction between two large subfields, both of which share \n",
    "many methods but typically pursue different goals: applied linguistics, commonly referred\n",
    "to as Natural Language Processing (NLP), and empirical linguistics. Of course\n",
    "the term NLP is often used interchangably with empirical linguistics, due to the large\n",
    "overlap of methods. But these fields pursue distinctive goals.\n",
    "\n",
    "**Natural Language Processing** is often aimed at using quantitative methods to build useful \n",
    "tools that solve real-world problems. Think of tools such as Google search or Alexa. NLP methods\n",
    "are often theory-neutral when it comes to language science. The success of an NLP project is best \n",
    "judged by the question, *does it work?*\n",
    "\n",
    "**Empirical linguistics** is aimed at using quantitative methods to formulate and test linguistic \n",
    "hypthotheses with real-world data. This field often relies on data procured from digital corpora or \n",
    "from human respondants. That data is then analyzed with a given research question in mind. The success\n",
    "of an empirical linguistic project is best judged by the question, *what question does it answer?*\n",
    "\n",
    "**The following chapters will primarily focus on the empirical linguistic approach from the perspective\n",
    "of corpus linguistics.**  A lot of theoretical issues can be addressed via empirical corpus \n",
    "linguistics, including (but not limited to, see Stefanowitsch 2020 below): \n",
    "\n",
    "* [lexical/grammatical collocation](https://www.researchgate.net/publication/37929828_Collostructions_Investigating_the_interaction_of_words_and_constructions)\n",
    "* [grammar, syntax, semantics](http://wwwling.arts.kuleuven.be/qlvl/prints/levshina_heylen_9998draft_radically_data-driven_construction.pdf)\n",
    "* [metaphor](https://benjamins.com/catalog/scl.2)\n",
    "* [intertextuality](https://www.springer.com/gp/book/9783030234133)\n",
    "* [historical linguistics](http://lingpy.org)\n",
    "* and more\n",
    "\n",
    "The chapters of this tutorial will not cover these areas individually, but will rather focus on a \n",
    "few methods that are widely applicable to these various areas.\n",
    "\n",
    "A major source for this chapter that provides a more comprehensive overview of the relevant\n",
    "issues and methods is *Corpus Linguistics* (2020) by Anatol Stefanowitsch. A free copy of the \n",
    "book can be downloaded by following the image link below.\n",
    "\n",
    "<a href=\"https://langsci-press.org/catalog/book/148\"><img src=\"https://langsci-press.org/$$$call$$$/submission/cover/cover?submissionId=148&random=1485eaad8671e2ef\" height=300 width=300></a>\n",
    "\n",
    "Additional resources are given/cited below. Even though they are written for R, the methods are \n",
    "equally applicable for Python and only require some \"translation\" of R dataframes to\n",
    "Pandas DataFrames.\n",
    "\n",
    "* [Harald Baayen, *Analyzing Linguistic Data: A practical introduction to statistics using R*, 2008.](https://www.researchgate.net/publication/311509723_Analyzing_Linguistic_Data_A_practical_introduction_to_statistics_using_R)\n",
    "* [Natalia Levshina, *How to do Linguistics with R*, 2015.](https://benjamins.com/catalog/z.195)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating Linguistic Hypotheses\n",
    "\n",
    "In empirical linguistics, we seek to convert theoretical questions into \n",
    "testable hypotheses. In the natural sciences, the normal objective is to \n",
    "**disprove** a hypothesis rather than prove it. Have a look at the image below. \n",
    "We will pretend this is our dataset which tells us something about \"swans\".\n",
    "\n",
    "<img src=https://i.dailymail.co.uk/i/pix/2014/03/13/article-0-1C40765B00000578-240_964x614.jpg height=600 width=600>\n",
    "\n",
    "Based on our dataset, we formulate the following existential hypothesis:\n",
    "\n",
    "> \"All swans are white.\"\n",
    "\n",
    "Is this hypothesis warranted? According to our limited dataset, we might say based on \n",
    "[inductive logic](https://plato.stanford.edu/entries/logic-inductive/) that our hypothesis\n",
    "is not only warranted but true! But this kind of thinking, as Karl Popper showed (1959) is \n",
    "prone to error. It would only take a single observation of a black swan to disprove the \n",
    "hypothesis. In fact, **the primary objective of the scientific method is to disprove a\n",
    "hypothesis**. In the natural sciences, this is most often done through experimentation. \n",
    "\n",
    "As Stefanowitsch points out (2020: 77-93), the normal goal of finding counter examples to\n",
    "a hypothesis is particularly problematic for corpus linguistics. This is due to the near \n",
    "infinite linguistic variation and diversity found in human language. A corpus is always \n",
    "a very limited snapshot of a particular speech situation. \n",
    "\n",
    "**In linguistics, hypotheses tend to be formulated in terms of statistical tendencies \n",
    "rather than in existential terms.** For instance (examples from Stefanowitsch 2020: 75):\n",
    "\n",
    "> \"X's tend to be Y\"\n",
    "\n",
    "or \n",
    "\n",
    "> \"Z's prefer Y\" \n",
    "\n",
    "We can say formally:\n",
    "\n",
    "    A linguistic hypothesis is a \"statement postulating relationships between\n",
    "    [two or more] constructs\". (Stefanowitsch 2020: 64)\n",
    "    \n",
    "Here are two examples, taken from Stefanowitsch (2020):\n",
    "\n",
    "> Most occurrences of the suffix *-wards* are British English\\\n",
    "> Most occurrences of the suffix *-ward* are American English\n",
    "\n",
    "In each hypothesis, we have two constructs: 1) \"a suffix\" consisting\n",
    "of a sequence that is either *-wards* or *-ward*,  and 2) a dialect \n",
    "of English, either American or British. Note that we assume the existence\n",
    "of these constructs when formulating these hypotheses.\n",
    "\n",
    "There are ways of inductively testing for natural patterns in data, such as\n",
    "unsupervised clustering methods, that provide interesting avenues for testing the validity \n",
    "of our constructs. But the majority of research questions will revolve around\n",
    "constructs already assumed to exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalizing a Hypothesis\n",
    "\n",
    "After we formulate a hypothesis that can be tested with statistical\n",
    "means, we must then decide how to go about gathering and arranging \n",
    "our data. This requires us to \"operationalize\" our terms. \n",
    "\n",
    "For instance, Stefanowitsch provides a useful example of the concept \n",
    "of 'hardness', defined in a dictionary as (2020: 77):\n",
    "\n",
    "> FIRM TO TOUCH firm, stiff and difficult to press down, break, or cut\n",
    "\n",
    "This definition fits our \"experiential\" knowledge of 'hardness'. But more \n",
    "objective means of hardness are needed if we're going to, for instance,\n",
    "compare how rugged two different kinds of phone screens are. This requires\n",
    "us to condense the experiential / cultural concept of hardness down into a \n",
    "procedure that can be accurately reproduced in various situations. One such example\n",
    "is the Vickers Hardness Test (Stefanowitsch 2020: 78):\n",
    "\n",
    "$HV = \\frac{0.102 \\times F}{A}$\n",
    "\n",
    "Where $F$ is the load in newtons, $A$ is the surface of the indentation of an \n",
    "object pressing on the material, and $0.102$ is a constant that converts\n",
    "newtons into a kilopond.\n",
    "\n",
    "In linguistic research we operationalize complex entities into a series of operations.\n",
    "For instance, say we are trying to measure the behavior of verbs within a text corpus. \n",
    "We cannot simply define something like a 'verb' intuitively, since our Python code has \n",
    "no concept of 'verb'—it cannot think about actions and movements...it cannot think at all!\n",
    "So, then, we are forced to operationalize what is a very intricate concept into\n",
    "something we can produce. For instance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_verb(string):\n",
    "    \"\"\"Test whether a string is a verb.\"\"\"\n",
    "    if string.endswith('ing'):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, of course, is by no means a comprehensive definition of a verb. But only an example. Even if \n",
    "we manually annotate all cases of verbs, we then must define verb as some entity. For instance,\n",
    "see the following mini-dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = [\n",
    "    ('went', 'v'),\n",
    "    ('going', 'v')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the small dataset, we've marked a 'v' for each word what we think is a verb. That 'v' string is not, \n",
    "of course itself a 'verb', rather it is a simplified formalism meant to point to a 'verb'. In fact,\n",
    "the 'v' is simply *our interpretation* of a verb. \n",
    "\n",
    "Another example might be operationalizing the concept of word order. This is often done already in\n",
    "linguistic research, where word order is simplified to:\n",
    "\n",
    "$word\\_order = SV$\n",
    "\n",
    "or \n",
    "\n",
    "$word\\_order = VS$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering and Structuring Data\n",
    "\n",
    "### Dependent versus Independent Variables\n",
    "\n",
    "Remember our definition of a linguistic hypothesis as:\n",
    "\n",
    "    a \"statement postulating relationships between\n",
    "    [two or more] constructs\". (Stefanowitsch 2020: 64)\n",
    "    \n",
    " We make a further specification, again following Stefanowitsch 2020: 64,\n",
    " \n",
    "     the construct \"we want to explain\" is called the dependent\n",
    "     variable; the construct \"we believe provides an explanation\"\n",
    "     is called the independent variable.\n",
    "     \n",
    "For example, take the following hypothesis:\n",
    "\n",
    "> Present tense verbs are used more frequently in speech than in prose.\n",
    "\n",
    "We have two independent variables: \"speech\" and \"prose\". These are the \n",
    "variables we assume \"explain\" (in a limited sense) how often a present \n",
    "tense verb is used in the corpus. The \"present tense verb\" is therefore\n",
    "the dependent variable.\n",
    "\n",
    "### Structuring A Dataset\n",
    "\n",
    "**Always structure your data in the raw data table format consisting of observations\n",
    "in rows, and features of those observations in columns (Stefanowitsch 2020: 137)**.\n",
    "This is the standard in data science, and is constructed so as to preserve your\n",
    "data in a reproducible format.\n",
    "\n",
    "|  | feature 1 | feature 2 |  feature 3 | ... |\n",
    "|--|--|--|--|--|\n",
    "| observation 1 | \n",
    "| observation 2 | \n",
    "| observation 3 | \n",
    "| ... | \n",
    "\n",
    "The observation is the primary entity you're analyzing. For example, if you're analyzing\n",
    "verbs, each observation is a verb in the corpus, and each feature is a particular feature\n",
    "of that verb or of its context.\n",
    "\n",
    "**NEVER store your data in summarized format, i.e. where each row and column is a sum or \n",
    "cross tabulation of co-occurring features (Stefanowitsch 2020: 137).** An example of a \n",
    "summarized format is given below:\n",
    "\n",
    "|            | subject | object |\n",
    "|------------|---------|--------|\n",
    "| verb_stem1 | 123     | 225    |\n",
    "| verb_stem2 | 521     | 12     |\n",
    "\n",
    "This format \"irreversibly destroys the relationship between the corpus hits and \n",
    "the different annotations applied to them\" (Stefanowitsch 2020: 138). This format\n",
    "is fine (and necessary) for analysis and for displaying results! But it should\n",
    "never be the primary way you store your data. Always use the raw data table format. \n",
    "\n",
    "Furthermore, as we shall discuss below, always publish your raw data alongside \n",
    "your research. Preferably in an open medium intended for data such as Github \n",
    "or Zenodo.\n",
    "\n",
    "### Dataset Illustration\n",
    "\n",
    "Let's create an example of a raw data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we're researching sentence tendencies in a corpus. Here's a snapshot of [that corpus](https://github.com/CambridgeSemiticsLab/nena_corpus/blob/master/nena/0.01/Barwar/The%20Bear%20and%20the%20Fox.nena), \n",
    "which is Neo-Aramaic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_bear_and_fox = \"\"\"\n",
    "xà-yomaˈ də́bba xðára-wawa gu-ṭuràne.ˈ tfíqla b-xa-tèla.ˈ ʾo-tèlaˈ mə́re há\n",
    "lɛ̀kət zála,ˈ ya-gáni də̀bba?ˈ b-álaha xðárən báθər rə̀sqiˈ bálki xa-mə́ndi táfəq\n",
    "bìyiˈ ṱ-àxlən mə́ndi,ˈ ʾáxxa m-tàmma.ˈ ʾámər ma-lat-ðáya b-gánəx qàrθɛla?ˈ dáx-it\n",
    "jwàja?ˈ ʾáxxa l-tàmma.ˈ mə́ra làˈ ču-qárθa lɛ̀la ʾáxxa.ˈ mə́re ʾən-bằyətˈ\n",
    "zaqrə̀nnəxˈ xa-qərṭàla,ˈ sàla y-amrə́xle,ˈ xa-sàla,ˈ ṭla-sə̀twaˈ ta-t-lá qɛràti.ˈ\n",
    "t-lá-hoya qàrθa-ʾəlləx,ˈ t-lá-hawe tàlga-ʾəlləx.ˈ hóla rəš-ṭùra,ˈ tìwtɛla.ˈ mə́re\n",
    "də-hàyyo!ˈ qímɛle múθya tùreˈ ʾu-č̭ənnək̭ɛ́ra díya di-di-dì,ˈ mə́re túgən ʾáti\n",
    "gàwe.ˈ tìwtɛla.ˈ zqìrəllaˈ hal-làxxa.ˈ mə́ra də-klìgən!ˈ pàlṭən m-gáwe.ˈ mə́re làˈ\n",
    "ta-ṱ-óðən qắpəx ʾàp ṭla-réšəx,ˈ baʿdḕn pàlṭət.ˈ qìmɛle,ˈ xθìməlle.ˈ kúlle\n",
    "zqìrəlle-wˈ píštɛla hádəx də́bba gàwe.ˈ ʾu-maxéla ʾáqle díye ʾə̀lla.ˈ ʾu-ṣàlya.ˈ\n",
    "nàpla,ˈ ṣálya-wˈ ṣàlya,ˈ ṣàlyaˈ b-o-ṭùraˈ hal-šə́ttət hə̀nna,ˈ gu-rawùlta.ˈ ṣléla\n",
    "tàma.ˈ ʾína kúlla gɛ́rme díya šmìṭe.ˈ píšta be-sarùber,ˈ hat-šmíṭle ʾɛ-sála\n",
    "t-wéwa zqìrəlla.ˈ kúlla šmìṭla.ˈ módi wìdle?ˈ ʾaw-téla rìqle.ˈ téla rìqle,ˈ\n",
    "ʾáy hédi-hedi qìmla.ˈ xà-yoma,ˈ trè,ˈ ṭḷàθa,ˈ bar-píšla spày,ˈ zìlla.ˈ zílla\n",
    "báre báre dìye.ˈ wírre gu-xa-bòya.ˈ wírre gu-xà ʾisára.ˈ \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we split the text into sentences along punctuation lines. We use the period + intonation\n",
    "boundary (ˈ) to do this. For this example, we'll leave the line markers e.g. (1) and simply consider that part of the\n",
    "sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = the_bear_and_fox.split('.ˈ')\n",
    "\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each sentence is a single observation**. Now we'll construct the dataset by\n",
    "gathering features on those observations. We'll gather the following features:\n",
    "\n",
    "* number of words in sentence (segmented by space or -)\n",
    "* number of stress groups in sentence (segmented by space only)\n",
    "* whether a sentence contains a segement marked with a comma (,)\n",
    "* the first word of the sentence\n",
    "\n",
    "Pay attention to how we store each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_table = [] # <- put data here\n",
    "\n",
    "# for each sentence, create a new row\n",
    "# in the table by appending a list of\n",
    "# feature values to the raw_data_table list\n",
    "\n",
    "for sentence in sentences:\n",
    "    \n",
    "    # clean the sentence of newlines \\n and skip any empty sentences\n",
    "    sentence = sentence.replace('\\n', '').strip()\n",
    "    if not sentence:\n",
    "        continue\n",
    "    \n",
    "    # gather the data to store\n",
    "    words = re.split(' |-', sentence) # use re to split by either space or (|) dash -\n",
    "    stress_groups = sentence.split(' ')\n",
    "    has_comma = ',' in sentence\n",
    "    first_word = words[0]\n",
    "    \n",
    "    # package the data into a list which will become a \"row\" in the raw_data_table\n",
    "    sentence_data = [len(words), len(stress_groups), has_comma, first_word]\n",
    "    \n",
    "    # add the \"row\" into the table by appending the data list to the raw_data_table list\n",
    "    raw_data_table.append(sentence_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_table[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we convert the dataset into a DataFrame so we can do statistical analysis\n",
    "with it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df = pd.DataFrame(raw_data_table, columns=['n_words', 'n_stress', 'has_comma', 'first_word'])\n",
    "\n",
    "raw_data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the data for later use and also for archiving by exporting it as a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df.to_csv('example_raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Science & Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since scientific research should be falsifiable, it must also be reproducible.\n",
    "In other words, when you report the results of your research, anyone should be\n",
    "able to pick up your data and reproduce the same results you found. Often in linguistics\n",
    "this is not the case. We frequently see summary data tables with counts of observed\n",
    "features, but no provision of the data that produced those counts.\n",
    "\n",
    "**Your results are only as good as the accessibility of your data**. If you publish really \n",
    "nice results, but not the data itself, those results are of limited or no value to the \n",
    "advancement of the field. As such, proprietary or protected data has lesser value to good\n",
    "research than data that is freely available.\n",
    "\n",
    "Here are just a couple of avenues you can choose for publishing your data online. They are \n",
    "relatively easy to use. Upload your data to one of these archives in the form of a\n",
    "raw data table, either as a CSV file or with a format that can preserve that structure of\n",
    "a table. Then you can provide a hyperlink to accompany your paper/article to direct\n",
    "your readers to the full form of the data.\n",
    "\n",
    "* [Get started with Github](https://help.github.com/en/github/getting-started-with-github)\n",
    "* [About Zenodo](https://about.zenodo.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
