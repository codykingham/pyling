{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material is copied (possibily with some modifications) from the [Python for Text-Analysis course](https://github.com/cltl/python-for-text-analysis/tree/master/Chapters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 24: NLP Pipeline with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way to go! You have already learnt a lot of essential components of the Python language. Being able to deal with data structures, import packages, build your own functions and operate with files is not only essential for most tasks in Python, but also a prerequisite for text analysis. We have applied some common preprocessing steps like casefolding/lowercasing, punctuation removal and stemming/lemmatization. Did you know that there are some very useful NLP packages and modules that do some of these steps? One that is often used in text analysis is the Python package **NLTK (the Natural Language Toolkit)**.\n",
    "\n",
    "### At the end of this chapter, you will be able to:\n",
    "* have an idea of the NLP tasks that constitute an NLP pipeline\n",
    "* use the functions of the NLTK module to manipulate the content of files for NLP purposes (e.g. sentence splitting, tokenization, POS-tagging, and lemmatization);\n",
    "* do nesting of multiple for-loops or files\n",
    "\n",
    "### More NLP software for Python:\n",
    "* [NLTK](http://www.nltk.org/)\n",
    "* [SpaCy](https://spacy.io/)\n",
    "* [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/index.html)\n",
    "* [About Python NLP libraries](https://elitedatascience.com/python-nlp-libraries)\n",
    "\n",
    "\n",
    "If you have **questions** about this chapter, drop em in the Slack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 A short intro to text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many aspects of text we can (try to) analyze. Commonly used analyses conducted in Natural Lanugage Processing (**NLP**) are for instance:\n",
    "\n",
    "* determining the part of speech of words in a text (verb, noun, etc.)\n",
    "* analyzing the syntactic relations between words and phrases in a sentence (i.e. syntactic parsing)\n",
    "* analyzing which entities (people, organizations, locations) are mentioned in a text\n",
    "\n",
    "...and many more. Each of these aspects is addressed within its own **NLP task**. \n",
    "\n",
    "**The NLP pipeline**\n",
    "\n",
    "Usually, these tasks are carried out sequentially, because they depend on each other. For instance, we need to first tokenize the text (split it into words) in order to be able to assign part-of-speech tags to each word. This sequence is often called an **NLP pipeline**. For example, a general pipeline could consist of the components shown below (taken from [here](https://www.slideshare.net/YuriyGuts/natural-language-processing-nlp)) You can see the NLP pipeline of the NewsReader project [here](http://www.newsreader-project.eu/files/2014/02/SystemArchitecture.png). (you can ignore the middle part of the picture, and focus on the blue and green boxes in the outer row).\n",
    "\n",
    "<img src='images/nlp-pipeline.jpg'>\n",
    "\n",
    "In this chapter we will look into four simple NLP modules that are nevertheless very common in NLP: **tokenization, sentence splitting**, **lemmatization** and **POS tagging**. \n",
    "\n",
    "There are also more advanced processing modules out there - feel free to do some research yourself :-) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 The NLTK package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Processing Toolkit) is a module we can use for most fundamental aspects of natural language processing. There are many more advanced approaches out there, but it is a good way of getting started. \n",
    "\n",
    "Here we will show you how to use it for tokenization, sentence splitting, POS tagging, and lemmatization. These steps are necessary processing steps for most NLP tasks. \n",
    "\n",
    "We will first give you an overview of all tasks and then delve into each of them in more detail. \n",
    "\n",
    "Before we can use NLTK for the first time, we have to make sure it is downloaded and installed on our computer (some of you may have already done this). \n",
    "\n",
    "To install NLTK, please try to run the following 2 cells. If this does not work, please try and follow the [documentation](http://www.nltk.org/install.html). If you don't manage to get this to work, please ask for help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (0.14.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.5.7.tar.gz (696 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n",
      "Could not build wheels for nltk, since package 'wheel' is not installed.\n",
      "Could not build wheels for click, since package 'wheel' is not installed.\n",
      "Could not build wheels for joblib, since package 'wheel' is not installed.\n",
      "Could not build wheels for regex, since package 'wheel' is not installed.\n",
      "Installing collected packages: regex, tqdm, nltk\n",
      "    Running setup.py install for regex: started\n",
      "    Running setup.py install for regex: finished with status 'done'\n",
      "    Running setup.py install for nltk: started\n",
      "    Running setup.py install for nltk: finished with status 'done'\n",
      "Successfully installed nltk-3.5 regex-2020.5.7 tqdm-4.46.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/cody/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have installed and downloaded NLTK, let's look at an example of a simple NLP pipeline. In the following cell, you can observe how we tokenize raw text into tokens and setnences, perform part of speech tagging and lemmatize some of the tokens. Don't worry about the details just yet - we will go trhough them step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'sentence', 'is', 'used', 'for', 'illustrating', 'some', 'basic', 'NLP', 'tasks', '.', 'Language', 'is', 'awesome', '!']\n",
      "['This example sentence is used for illustrating some basic NLP tasks.', 'Language is awesome!']\n",
      "[('This', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('for', 'IN'), ('illustrating', 'VBG'), ('some', 'DT'), ('basic', 'JJ'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('.', '.'), ('Language', 'NN'), ('is', 'VBZ'), ('awesome', 'JJ'), ('!', '.')]\n",
      "use\n"
     ]
    }
   ],
   "source": [
    "text = \"This example sentence is used for illustrating some basic NLP tasks. Language is awesome!\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Sentence splitting\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# POS tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "lemma=lmtzr.lemmatize(tokens[4], 'v')\n",
    "\n",
    "# Printing all information\n",
    "print(tokens)\n",
    "print(sentences)\n",
    "print(tagged_tokens)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Tokenization and sentence splitting with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 `word_tokenize()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try tokenizing our Charlie story! First, we will open and read the file again and assign the file contents to the variable `content`. Then, we can call the `word_tokenize()` function from the `nltk` module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Charlie', 'Bucket', 'stared', 'around', 'the', 'gigantic', 'room', 'in', 'which', 'he', 'now', 'found', 'himself', '.', 'The', 'place', 'was', 'like', 'a', 'witch', '’', 's', 'kitchen', '!', 'All', 'about', 'him', 'black', 'metal', 'pots', 'were', 'boiling', 'and', 'bubbling', 'on', 'huge', 'stoves', ',', 'and', 'kettles', 'were', 'hissing', 'and', 'pans', 'were', 'sizzling', ',', 'and', 'strange', 'iron', 'machines', 'were', 'clanking', 'and', 'spluttering', ',', 'and', 'there', 'were', 'pipes', 'running', 'all', 'over', 'the', 'ceiling', 'and', 'walls', ',', 'and', 'the', 'whole', 'place', 'was', 'filled', 'with', 'smoke', 'and', 'steam', 'and', 'delicious', 'rich', 'smells', '.', 'Mr', 'Wonka', 'himself', 'had', 'suddenly', 'become', 'even', 'more', 'excited', 'than', 'usual', ',', 'and', 'anyone', 'could', 'see', 'that', 'this', 'was', 'the', 'room', 'he', 'loved', 'best', 'of', 'all', '.', 'He', 'was', 'hopping', 'about', 'among', 'the', 'saucepans', 'and', 'the', 'machines', 'like', 'a', 'child', 'among', 'his', 'Christmas', 'presents', ',', 'not', 'knowing', 'which', 'thing', 'to', 'look', 'at', 'first', '.', 'He', 'lifted', 'the', 'lid', 'from', 'a', 'huge', 'pot', 'and', 'took', 'a', 'sniff', ';', 'then', 'he', 'rushed', 'over', 'and', 'dipped', 'a', 'finger', 'into', 'a', 'barrel', 'of', 'sticky', 'yellow', 'stuff', 'and', 'had', 'a', 'taste', ';', 'then', 'he', 'skipped', 'across', 'to', 'one', 'of', 'the', 'machines', 'and', 'turned', 'half', 'a', 'dozen', 'knobs', 'this', 'way', 'and', 'that', ';', 'then', 'he', 'peered', 'anxiously', 'through', 'the', 'glass', 'door', 'of', 'a', 'gigantic', 'oven', ',', 'rubbing', 'his', 'hands', 'and', 'cackling', 'with', 'delight', 'at', 'what', 'he', 'saw', 'inside', '.', 'Then', 'he', 'ran', 'over', 'to', 'another', 'machine', ',', 'a', 'small', 'shiny', 'affair', 'that', 'kept', 'going', 'phut-phut-phut-phut-phut', ',', 'and', 'every', 'time', 'it', 'went', 'phut', ',', 'a', 'large', 'green', 'marble', 'dropped', 'out', 'of', 'it', 'into', 'a', 'basket', 'on', 'the', 'floor', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "\n",
    "tokens = nltk.word_tokenize(content)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we now have a list of all words in the text. The punctuation marks are also in the list, but as separate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 `sent_tokenize()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that NLTK can do for you is to split a text into sentences by using the `sent_tokenize()` function. We use it on the entire text (as a string):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Charlie Bucket stared around the gigantic room in which he now found himself.', 'The place was like a witch’s kitchen!', 'All about him black metal pots were boiling and bubbling on huge stoves, and kettles were hissing and pans were sizzling, and strange iron machines were clanking and spluttering, and there were pipes running all over the ceiling and walls, and the whole place was filled with smoke and steam and delicious rich smells.', 'Mr Wonka himself had suddenly become even more excited than usual, and anyone could see that this was the room he loved best of all.', 'He was hopping about among the saucepans and the machines like a child among his Christmas presents, not knowing which thing to look at first.', 'He lifted the lid from a huge pot and took a sniff; then he rushed over and dipped a finger into a barrel of sticky yellow stuff and had a taste; then he skipped across to one of the machines and turned half a dozen knobs this way and that; then he peered anxiously through the glass door of a gigantic oven, rubbing his hands and cackling with delight at what he saw inside.', 'Then he ran over to another machine, a small shiny affair that kept going phut-phut-phut-phut-phut, and every time it went phut, a large green marble dropped out of it into a basket on the floor.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(content)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do all sorts of cool things with these lists. For example, we can search for all words that have certain letters in them and add them to a list. Let's say we want to find all present participles in the text. We know that present participles end with *-ing*, so we can do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boiling', 'bubbling', 'hissing', 'sizzling', 'clanking', 'spluttering', 'running', 'ceiling', 'hopping', 'knowing', 'thing', 'rubbing', 'cackling', 'going']\n"
     ]
    }
   ],
   "source": [
    "# Open and read in file as a string, assign it to the variable `content`\n",
    "with open(\"../data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "# Split up entire text into tokens using word_tokenize():\n",
    "tokens = nltk.word_tokenize(content)\n",
    "\n",
    "# create an empty list to collect all words having the present participle -ing:\n",
    "present_participles = []\n",
    "\n",
    "# looking through all tokens\n",
    "for token in tokens:\n",
    "    # checking if a token ends with the present parciciple -ing\n",
    "    if token.endswith(\"ing\"):\n",
    "        # if the condition is met, add it to the list we created above (present_participles)\n",
    "        present_participles.append(token)\n",
    "        \n",
    "# Print the list to inspect it\n",
    "print(present_participles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good! We now have a list of words like *boiling*, *sizzling*, etc. However, we can see that there is one word in the list that actually is not a present participle (*ceiling*). Of course, also other words can end with *-ing*. So if we want to find all present participles, we have to come up with a smarter solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, NLTK comes to the rescue. Using the function `pos_tag()`, we can label each word in the text with its part of speech. \n",
    "\n",
    "To do pos-tagging, you first need to tokenize the text. We have already done this above, but we will repeat the steps here, so you get a sense of what an NLP pipeline may look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 `pos_tag()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how `pos_tag()` can be used, we can (as always) look at the documentation by using the `help()` function. As we can see, `pos_tag()` takes a tokenized text as input and returns a list of tuples in which the first element corresponds to the token and the second to the assigned pos-tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pos_tag in module nltk.tag:\n",
      "\n",
      "pos_tag(tokens, tagset=None, lang='eng')\n",
      "    Use NLTK's currently recommended part of speech tagger to\n",
      "    tag the given list of tokens.\n",
      "    \n",
      "        >>> from nltk.tag import pos_tag\n",
      "        >>> from nltk.tokenize import word_tokenize\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"))\n",
      "        [('John', 'NNP'), (\"'s\", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),\n",
      "        (\"n't\", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"), tagset='universal')\n",
      "        [('John', 'NOUN'), (\"'s\", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),\n",
      "        (\"n't\", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]\n",
      "    \n",
      "    NB. Use `pos_tag_sents()` for efficient tagging of more than one sentence.\n",
      "    \n",
      "    :param tokens: Sequence of tokens to be tagged\n",
      "    :type tokens: list(str)\n",
      "    :param tagset: the tagset to be used, e.g. universal, wsj, brown\n",
      "    :type tagset: str\n",
      "    :param lang: the ISO 639 code of the language, e.g. 'eng' for English, 'rus' for Russian\n",
      "    :type lang: str\n",
      "    :return: The tagged tokens\n",
      "    :rtype: list(tuple(str, str))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As always, we can start by reading the documentation:\n",
    "help(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Charlie', 'NNP'), ('Bucket', 'NNP'), ('stared', 'VBD'), ('around', 'IN'), ('the', 'DT'), ('gigantic', 'JJ'), ('room', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('he', 'PRP'), ('now', 'RB'), ('found', 'VBD'), ('himself', 'PRP'), ('.', '.'), ('The', 'DT'), ('place', 'NN'), ('was', 'VBD'), ('like', 'IN'), ('a', 'DT'), ('witch', 'NN'), ('’', 'NN'), ('s', 'NN'), ('kitchen', 'NN'), ('!', '.'), ('All', 'DT'), ('about', 'IN'), ('him', 'PRP'), ('black', 'JJ'), ('metal', 'NN'), ('pots', 'NNS'), ('were', 'VBD'), ('boiling', 'VBG'), ('and', 'CC'), ('bubbling', 'VBG'), ('on', 'IN'), ('huge', 'JJ'), ('stoves', 'NNS'), (',', ','), ('and', 'CC'), ('kettles', 'NNS'), ('were', 'VBD'), ('hissing', 'VBG'), ('and', 'CC'), ('pans', 'NNS'), ('were', 'VBD'), ('sizzling', 'VBG'), (',', ','), ('and', 'CC'), ('strange', 'JJ'), ('iron', 'NN'), ('machines', 'NNS'), ('were', 'VBD'), ('clanking', 'VBG'), ('and', 'CC'), ('spluttering', 'NN'), (',', ','), ('and', 'CC'), ('there', 'EX'), ('were', 'VBD'), ('pipes', 'NNS'), ('running', 'VBG'), ('all', 'DT'), ('over', 'IN'), ('the', 'DT'), ('ceiling', 'NN'), ('and', 'CC'), ('walls', 'NNS'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('whole', 'JJ'), ('place', 'NN'), ('was', 'VBD'), ('filled', 'VBN'), ('with', 'IN'), ('smoke', 'NN'), ('and', 'CC'), ('steam', 'NN'), ('and', 'CC'), ('delicious', 'JJ'), ('rich', 'JJ'), ('smells', 'NNS'), ('.', '.'), ('Mr', 'NNP'), ('Wonka', 'NNP'), ('himself', 'PRP'), ('had', 'VBD'), ('suddenly', 'RB'), ('become', 'VBN'), ('even', 'RB'), ('more', 'RBR'), ('excited', 'JJ'), ('than', 'IN'), ('usual', 'JJ'), (',', ','), ('and', 'CC'), ('anyone', 'NN'), ('could', 'MD'), ('see', 'VB'), ('that', 'IN'), ('this', 'DT'), ('was', 'VBD'), ('the', 'DT'), ('room', 'NN'), ('he', 'PRP'), ('loved', 'VBD'), ('best', 'JJS'), ('of', 'IN'), ('all', 'DT'), ('.', '.'), ('He', 'PRP'), ('was', 'VBD'), ('hopping', 'VBG'), ('about', 'IN'), ('among', 'IN'), ('the', 'DT'), ('saucepans', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('machines', 'NNS'), ('like', 'IN'), ('a', 'DT'), ('child', 'NN'), ('among', 'IN'), ('his', 'PRP$'), ('Christmas', 'NNP'), ('presents', 'NNS'), (',', ','), ('not', 'RB'), ('knowing', 'VBG'), ('which', 'WDT'), ('thing', 'NN'), ('to', 'TO'), ('look', 'VB'), ('at', 'IN'), ('first', 'RB'), ('.', '.'), ('He', 'PRP'), ('lifted', 'VBD'), ('the', 'DT'), ('lid', 'NN'), ('from', 'IN'), ('a', 'DT'), ('huge', 'JJ'), ('pot', 'NN'), ('and', 'CC'), ('took', 'VBD'), ('a', 'DT'), ('sniff', 'NN'), (';', ':'), ('then', 'RB'), ('he', 'PRP'), ('rushed', 'VBD'), ('over', 'RB'), ('and', 'CC'), ('dipped', 'VBD'), ('a', 'DT'), ('finger', 'NN'), ('into', 'IN'), ('a', 'DT'), ('barrel', 'NN'), ('of', 'IN'), ('sticky', 'JJ'), ('yellow', 'JJ'), ('stuff', 'NN'), ('and', 'CC'), ('had', 'VBD'), ('a', 'DT'), ('taste', 'NN'), (';', ':'), ('then', 'RB'), ('he', 'PRP'), ('skipped', 'VBD'), ('across', 'IN'), ('to', 'TO'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('machines', 'NNS'), ('and', 'CC'), ('turned', 'VBD'), ('half', 'PDT'), ('a', 'DT'), ('dozen', 'NN'), ('knobs', 'NN'), ('this', 'DT'), ('way', 'NN'), ('and', 'CC'), ('that', 'IN'), (';', ':'), ('then', 'RB'), ('he', 'PRP'), ('peered', 'VBD'), ('anxiously', 'RB'), ('through', 'IN'), ('the', 'DT'), ('glass', 'NN'), ('door', 'NN'), ('of', 'IN'), ('a', 'DT'), ('gigantic', 'JJ'), ('oven', 'NN'), (',', ','), ('rubbing', 'VBG'), ('his', 'PRP$'), ('hands', 'NNS'), ('and', 'CC'), ('cackling', 'VBG'), ('with', 'IN'), ('delight', 'NN'), ('at', 'IN'), ('what', 'WP'), ('he', 'PRP'), ('saw', 'VBD'), ('inside', 'RB'), ('.', '.'), ('Then', 'RB'), ('he', 'PRP'), ('ran', 'VBD'), ('over', 'RB'), ('to', 'TO'), ('another', 'DT'), ('machine', 'NN'), (',', ','), ('a', 'DT'), ('small', 'JJ'), ('shiny', 'NN'), ('affair', 'NN'), ('that', 'WDT'), ('kept', 'VBD'), ('going', 'VBG'), ('phut-phut-phut-phut-phut', 'NN'), (',', ','), ('and', 'CC'), ('every', 'DT'), ('time', 'NN'), ('it', 'PRP'), ('went', 'VBD'), ('phut', 'NN'), (',', ','), ('a', 'DT'), ('large', 'JJ'), ('green', 'JJ'), ('marble', 'NN'), ('dropped', 'VBD'), ('out', 'IN'), ('of', 'IN'), ('it', 'PRP'), ('into', 'IN'), ('a', 'DT'), ('basket', 'NN'), ('on', 'IN'), ('the', 'DT'), ('floor', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Open and read in file as a string, assign it to the variable `content`\n",
    "with open(\"../data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "# Split up entire text into tokens using word_tokenize():\n",
    "tokens = nltk.word_tokenize(content)\n",
    "\n",
    "# Apply pos tagging to the tokenized text\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# Inspect pos tags\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Working with POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, `pos_tag()` returns a list of tuples: The first element is the token, the second element indicates the part of speech (POS) of the token. \n",
    "\n",
    "This POS tagger uses the POS tag set of the Penn Treebank Project, which can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). For example, all tags starting with a V are used for verbs. \n",
    "\n",
    "We can now use this, for example, to identify all the verbs in a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stared', 'found', 'was', 'were', 'boiling', 'bubbling', 'were', 'hissing', 'were', 'sizzling', 'were', 'clanking', 'were', 'running', 'was', 'filled', 'had', 'become', 'was', 'loved', 'was', 'hopping', 'knowing', 'lifted', 'took', 'rushed', 'dipped', 'had', 'skipped', 'turned', 'peered', 'rubbing', 'cackling', 'saw', 'ran', 'kept', 'going', 'went', 'dropped']\n"
     ]
    }
   ],
   "source": [
    "# Open and read in file as a string, assign it to the variable `content`\n",
    "with open(\"../data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "# Apply tokenization and POS tagging\n",
    "tokens = nltk.word_tokenize(content)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# List of verb tags (i.e. tags we are interested in)\n",
    "verb_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "\n",
    "# Create an empty list to collect all verbs:\n",
    "verbs = []\n",
    "\n",
    "# Iterating over all tagged tokens\n",
    "for token, tag in tagged_tokens:\n",
    " \n",
    "    # Checking if the tag is any of the verb tags\n",
    "    if tag in verb_tags:\n",
    "        # if the condition is met, add it to the list we created above \n",
    "        verbs.append(token)\n",
    "        \n",
    "# Print the list to inspect it\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use NLTK to lemmatize words.\n",
    "\n",
    "The lemma of a word is the form of the word which is usually used in dictionary entries. This is useful for many NLP tasks, as it gives a better generalization than the strong a word appears in. To a computer, `cat` and `cats` are two completely different tokens, even though we know they are both forms of the same lemma. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 The WordNet lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the WordNetLemmatizer for this using the `lemmatize()` function. In the code below, we loop through the list of verbs, lemmatize each of the verbs, and add them to a new list called `verb_lemmas`. Again, we show all the processing steps (consider the comments in the code below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stared', 'found', 'was', 'were', 'boiling', 'bubbling', 'were', 'hissing', 'were', 'sizzling', 'were', 'clanking', 'were', 'running', 'was', 'filled', 'had', 'become', 'was', 'loved', 'was', 'hopping', 'knowing', 'lifted', 'took', 'rushed', 'dipped', 'had', 'skipped', 'turned', 'peered', 'rubbing', 'cackling', 'saw', 'ran', 'kept', 'going', 'went', 'dropped']\n",
      "['star', 'find', 'be', 'be', 'boil', 'bubble', 'be', 'hiss', 'be', 'sizzle', 'be', 'clank', 'be', 'run', 'be', 'fill', 'have', 'become', 'be', 'love', 'be', 'hop', 'know', 'lift', 'take', 'rush', 'dip', 'have', 'skip', 'turn', 'peer', 'rub', 'cackle', 'saw', 'run', 'keep', 'go', 'go', 'drop']\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "#### Process text as explained above ###\n",
    "\n",
    "with open(\"../data/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "tokens = nltk.word_tokenize(content)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "verb_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "verbs = []\n",
    "\n",
    "for token, tag in tagged_tokens:\n",
    "    if tag in verb_tags:\n",
    "        verbs.append(token)\n",
    "\n",
    "print(verbs)\n",
    "\n",
    "#############################################################################\n",
    "#### Use the list of verbs collected above to lemmatize all the verbs ###\n",
    "\n",
    "        \n",
    "# Instatiate a lemmatizer object\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# Create list to collect all the verb lemmas:\n",
    "verb_lemmas = []\n",
    "        \n",
    "for participle in verbs:\n",
    "    # For this lemmatizer, we need to indicate the POS of the word (in this case, v = verb)\n",
    "    lemma = lmtzr.lemmatize(participle, \"v\") \n",
    "    verb_lemmas.append(lemma)\n",
    "print(verb_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about the wordnet lemmatizer:** \n",
    "\n",
    "We need to specify a POS tag to the WordNet lemmatizer, in a WordNet format (\"n\" for noun, \"v\" for verb, \"a\" for adjective). If we do not indicate the Part-of-Speech tag, the WordNet lemmatizer thinks it is a noun (this is the default value for its part-of-speech). See the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun in conjugated form: building\n",
      "Default lemmatization: building\n",
      "Lemmatization as a verb: build\n",
      "Lemmatization as a noun: building\n",
      "\n",
      "Noun in conjugated form: applications\n",
      "Default lemmatization: application\n",
      "Lemmatization as a verb: applications\n",
      "Lemmatization as a noun: application\n",
      "\n",
      "Noun in conjugated form: leafs\n",
      "Default lemmatization: leaf\n",
      "Lemmatization as a verb: leaf\n",
      "Lemmatization as a noun: leaf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_nouns = ('building', 'applications', 'leafs')\n",
    "for n in test_nouns:\n",
    "    print(f\"Noun in conjugated form: {n}\")\n",
    "    default_lemma=lmtzr.lemmatize(n) # default lemmatization, without specifying POS, n is interpretted as a noun!\n",
    "    print(f\"Default lemmatization: {default_lemma}\")\n",
    "    verb_lemma=lmtzr.lemmatize(n, 'v')\n",
    "    print(f\"Lemmatization as a verb: {verb_lemma}\")\n",
    "    noun_lemma=lmtzr.lemmatize(n, 'n')\n",
    "    print(f\"Lemmatization as a noun: {noun_lemma}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb in conjugated form: grew\n",
      "Default lemmatization: grew\n",
      "Lemmatization as a verb: grow\n",
      "Lemmatization as a noun: grew\n",
      "\n",
      "Verb in conjugated form: standing\n",
      "Default lemmatization: standing\n",
      "Lemmatization as a verb: stand\n",
      "Lemmatization as a noun: standing\n",
      "\n",
      "Verb in conjugated form: plays\n",
      "Default lemmatization: play\n",
      "Lemmatization as a verb: play\n",
      "Lemmatization as a noun: play\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_verbs=('grew', 'standing', 'plays')\n",
    "for v in test_verbs:\n",
    "    print(f\"Verb in conjugated form: {v}\")\n",
    "    default_lemma=lmtzr.lemmatize(v) # default lemmatization, without specifying POS, v is interpretted as a noun!\n",
    "    print(f\"Default lemmatization: {default_lemma}\")\n",
    "    verb_lemma=lmtzr.lemmatize(v, 'v')\n",
    "    print(f\"Lemmatization as a verb: {verb_lemma}\")\n",
    "    noun_lemma=lmtzr.lemmatize(v, 'n')\n",
    "    print(f\"Lemmatization as a noun: {noun_lemma}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Combining NLTK POS tags with the WordNet lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordNet lemmatizer assumes every word is a noun unless specified diferently. We need to be careful and specify the POS tag because otherwise we will end up with wrong lemmatization such as the cases shown in the past two cells. For example, by default WordNet thinks that \"grew\" is a noun, and it will not lemmatize it as a past-tense verb.\n",
    "\n",
    "Luckily, we learned that we can also automatically infer the POS tags for each word. We can use these automatic POS tags as input to our lemmatizer to improve its accuracy for non-nouns. As an intermediate step, we need to translate the POS tags that we get from our POS tagger (this are according to the Penn TreeBank classification) to WordNet POS tags. Here is an example of how to lemmatize your words in a proper way, accounting for different POS tags (you can also read [this discussion](https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Charlie', 'Bucket', 'star', 'around', 'the', 'gigantic', 'room', 'in', 'which', 'he', 'now', 'find', 'himself', '.', 'The', 'place', 'be', 'like', 'a', 'witch', '’', 's', 'kitchen', '!', 'All', 'about', 'him', 'black', 'metal', 'pot', 'be', 'boil', 'and', 'bubble', 'on', 'huge', 'stove', ',', 'and', 'kettle', 'be', 'hiss', 'and', 'pan', 'be', 'sizzle', ',', 'and', 'strange', 'iron', 'machine', 'be', 'clank', 'and', 'spluttering', ',', 'and', 'there', 'be', 'pipe', 'run', 'all', 'over', 'the', 'ceiling', 'and', 'wall', ',', 'and', 'the', 'whole', 'place', 'be', 'fill', 'with', 'smoke', 'and', 'steam', 'and', 'delicious', 'rich', 'smell', '.', 'Mr', 'Wonka', 'himself', 'have', 'suddenly', 'become', 'even', 'more', 'excited', 'than', 'usual', ',', 'and', 'anyone', 'could', 'see', 'that', 'this', 'be', 'the', 'room', 'he', 'love', 'best', 'of', 'all', '.', 'He', 'be', 'hop', 'about', 'among', 'the', 'saucepan', 'and', 'the', 'machine', 'like', 'a', 'child', 'among', 'his', 'Christmas', 'present', ',', 'not', 'know', 'which', 'thing', 'to', 'look', 'at', 'first', '.', 'He', 'lift', 'the', 'lid', 'from', 'a', 'huge', 'pot', 'and', 'take', 'a', 'sniff', ';', 'then', 'he', 'rush', 'over', 'and', 'dip', 'a', 'finger', 'into', 'a', 'barrel', 'of', 'sticky', 'yellow', 'stuff', 'and', 'have', 'a', 'taste', ';', 'then', 'he', 'skip', 'across', 'to', 'one', 'of', 'the', 'machine', 'and', 'turn', 'half', 'a', 'dozen', 'knob', 'this', 'way', 'and', 'that', ';', 'then', 'he', 'peer', 'anxiously', 'through', 'the', 'glass', 'door', 'of', 'a', 'gigantic', 'oven', ',', 'rub', 'his', 'hand', 'and', 'cackle', 'with', 'delight', 'at', 'what', 'he', 'saw', 'inside', '.', 'Then', 'he', 'run', 'over', 'to', 'another', 'machine', ',', 'a', 'small', 'shiny', 'affair', 'that', 'keep', 'go', 'phut-phut-phut-phut-phut', ',', 'and', 'every', 'time', 'it', 'go', 'phut', ',', 'a', 'large', 'green', 'marble', 'drop', 'out', 'of', 'it', 'into', 'a', 'basket', 'on', 'the', 'floor', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing (the proper way, accounting for different POS tags)\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "# We can write a general function to translate penn tree bank tags to wordnet tags\n",
    "def penn_to_wn(penn_tag):\n",
    "    \"\"\"\n",
    "    Returns the corresponding WordNet POS tag for a Penn TreeBank POS tag.\n",
    "    \"\"\"\n",
    "    if penn_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        wn_tag = wn.NOUN\n",
    "    elif penn_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        wn_tag = wn.VERB\n",
    "    elif penn_tag in ['RB', 'RBR', 'RBS']:\n",
    "        wn_tag = wn.ADV\n",
    "    elif penn_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        wn_tag = wn.ADJ\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    return wn_tag\n",
    "\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# create empty list to collect lemmas\n",
    "lemmas = list()\n",
    "\n",
    "# We use the tagged tokens we collected above and loop through the list of tuples\n",
    "for token, pos in tagged_tokens:\n",
    "    # convert Penn Treebank POS tag to WordNet POS tag\n",
    "    wn_tag = penn_to_wn(pos) \n",
    "    # we check if a wordnet tag was assigned\n",
    "    if not wn_tag == None:\n",
    "        # we lemmatize using the translated wordnet tag\n",
    "        lemma = lmtzr.lemmatize(token, wn_tag)\n",
    "    else:\n",
    "        # if there is no wordnet tag, we apply default lemmatization\n",
    "        lemma = lmtzr.lemmatize(token)\n",
    "    # add lemmas to list\n",
    "    lemmas.append(lemma)\n",
    "    \n",
    "# Inspect lemmas by printing them\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Nesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we typically used a single for-loop or we were opening a single file at a time. In Python (and most programming languages), one can **nest** multiple loops or files in one another. For instance, we can use one (external) for-loop to iterate through files, and then for each file iterate through all its sentences (internal for-loop). As we have learned above, `glob` is a convenient way of creating a list of files. \n",
    "\n",
    "You might think: can we stretch this on more levels? Iterate through files, then iterate through the sentences in these files, then iterate through each word in these sentences, then iterate through each letter in these words, etc.  This is possible. Python (and most programming languages) allow you to perform nesting with (in theory) as many loops as you want. Keep in mind that nesting too much will eventually cause computational problems, but this depends also on the size of your data.\n",
    "\n",
    "In the code below, we want get an idea of the number and length of the sentences in the texts stored in the `../data/Dreams` directory. We do this by creating two for loops: We iterate over all the files in the directory (loop 1), apply sentence tokenization and iterate over all the sentences in the file (loop 2).\n",
    "\n",
    "Look at the code and comments below to figure out what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: File ../data/Dreams/vickie8.txt has 17 sentences\n",
      "Sentence 1 has 13 tokens\n",
      "Sentence 2 has 15 tokens\n",
      "Sentence 3 has 9 tokens\n",
      "Sentence 4 has 11 tokens\n",
      "Sentence 5 has 8 tokens\n",
      "Sentence 6 has 13 tokens\n",
      "Sentence 7 has 8 tokens\n",
      "Sentence 8 has 10 tokens\n",
      "Sentence 9 has 18 tokens\n",
      "Sentence 10 has 7 tokens\n",
      "Sentence 11 has 19 tokens\n",
      "Sentence 12 has 10 tokens\n",
      "Sentence 13 has 11 tokens\n",
      "Sentence 14 has 5 tokens\n",
      "Sentence 15 has 12 tokens\n",
      "Sentence 16 has 9 tokens\n",
      "Sentence 17 has 5 tokens\n",
      "\n",
      "INFO: File ../data/Dreams/vickie9.txt has 9 sentences\n",
      "Sentence 1 has 11 tokens\n",
      "Sentence 2 has 12 tokens\n",
      "Sentence 3 has 8 tokens\n",
      "Sentence 4 has 9 tokens\n",
      "Sentence 5 has 20 tokens\n",
      "Sentence 6 has 6 tokens\n",
      "Sentence 7 has 6 tokens\n",
      "Sentence 8 has 13 tokens\n",
      "Sentence 9 has 16 tokens\n",
      "\n",
      "INFO: File ../data/Dreams/vickie10.txt has 16 sentences\n",
      "Sentence 1 has 18 tokens\n",
      "Sentence 2 has 5 tokens\n",
      "Sentence 3 has 15 tokens\n",
      "Sentence 4 has 19 tokens\n",
      "Sentence 5 has 11 tokens\n",
      "Sentence 6 has 13 tokens\n",
      "Sentence 7 has 18 tokens\n",
      "Sentence 8 has 9 tokens\n",
      "Sentence 9 has 5 tokens\n",
      "Sentence 10 has 13 tokens\n",
      "Sentence 11 has 10 tokens\n",
      "Sentence 12 has 13 tokens\n",
      "Sentence 13 has 10 tokens\n",
      "Sentence 14 has 7 tokens\n",
      "Sentence 15 has 28 tokens\n",
      "Sentence 16 has 8 tokens\n",
      "\n",
      "INFO: File ../data/Dreams/vickie7.txt has 22 sentences\n",
      "Sentence 1 has 15 tokens\n",
      "Sentence 2 has 8 tokens\n",
      "Sentence 3 has 14 tokens\n",
      "Sentence 4 has 18 tokens\n",
      "Sentence 5 has 10 tokens\n",
      "Sentence 6 has 6 tokens\n",
      "Sentence 7 has 13 tokens\n",
      "Sentence 8 has 19 tokens\n",
      "Sentence 9 has 7 tokens\n",
      "Sentence 10 has 6 tokens\n",
      "Sentence 11 has 7 tokens\n",
      "Sentence 12 has 11 tokens\n",
      "Sentence 13 has 6 tokens\n",
      "Sentence 14 has 17 tokens\n",
      "Sentence 15 has 4 tokens\n",
      "Sentence 16 has 7 tokens\n",
      "Sentence 17 has 11 tokens\n",
      "Sentence 18 has 13 tokens\n",
      "Sentence 19 has 8 tokens\n",
      "Sentence 20 has 6 tokens\n",
      "Sentence 21 has 13 tokens\n",
      "Sentence 22 has 19 tokens\n",
      "\n",
      "INFO: File ../data/Dreams/vickie6.txt has 17 sentences\n",
      "Sentence 1 has 6 tokens\n",
      "Sentence 2 has 7 tokens\n",
      "Sentence 3 has 23 tokens\n",
      "Sentence 4 has 13 tokens\n",
      "Sentence 5 has 12 tokens\n",
      "Sentence 6 has 13 tokens\n",
      "Sentence 7 has 10 tokens\n",
      "Sentence 8 has 8 tokens\n",
      "Sentence 9 has 6 tokens\n",
      "Sentence 10 has 17 tokens\n",
      "Sentence 11 has 7 tokens\n",
      "Sentence 12 has 19 tokens\n",
      "Sentence 13 has 4 tokens\n",
      "Sentence 14 has 10 tokens\n",
      "Sentence 15 has 5 tokens\n",
      "Sentence 16 has 11 tokens\n",
      "Sentence 17 has 11 tokens\n",
      "\n",
      "INFO: File ../data/Dreams/vickie4.txt has 12 sentences\n",
      "Sentence 1 has 11 tokens\n",
      "Sentence 2 has 18 tokens\n",
      "Sentence 3 has 6 tokens\n",
      "Sentence 4 has 13 tokens\n",
      "Sentence 5 has 11 tokens\n",
      "Sentence 6 has 15 tokens\n",
      "Sentence 7 has 19 tokens\n",
      "Sentence 8 has 8 tokens\n",
      "Sentence 9 has 8 tokens\n",
      "Sentence 10 has 26 tokens\n",
      "Sentence 11 has 8 tokens\n",
      "Sentence 12 has 13 tokens\n",
      "\n",
      "INFO: File ../data/Dreams/vickie5.txt has 9 sentences\n",
      "Sentence 1 has 14 tokens\n",
      "Sentence 2 has 8 tokens\n",
      "Sentence 3 has 10 tokens\n",
      "Sentence 4 has 6 tokens\n",
      "Sentence 5 has 15 tokens\n",
      "Sentence 6 has 15 tokens\n",
      "Sentence 7 has 9 tokens\n",
      "Sentence 8 has 13 tokens\n",
      "Sentence 9 has 10 tokens\n",
      "\n",
      "INFO: File ../data/Dreams/vickie1.txt has 17 sentences\n",
      "Sentence 1 has 10 tokens\n",
      "Sentence 2 has 9 tokens\n",
      "Sentence 3 has 8 tokens\n",
      "Sentence 4 has 12 tokens\n",
      "Sentence 5 has 13 tokens\n",
      "Sentence 6 has 14 tokens\n",
      "Sentence 7 has 6 tokens\n",
      "Sentence 8 has 8 tokens\n",
      "Sentence 9 has 5 tokens\n",
      "Sentence 10 has 12 tokens\n",
      "Sentence 11 has 9 tokens\n",
      "Sentence 12 has 14 tokens\n",
      "Sentence 13 has 9 tokens\n",
      "Sentence 14 has 21 tokens\n",
      "Sentence 15 has 15 tokens\n",
      "Sentence 16 has 13 tokens\n",
      "Sentence 17 has 8 tokens\n",
      "\n",
      "INFO: File ../data/Dreams/vickie2.txt has 12 sentences\n",
      "Sentence 1 has 13 tokens\n",
      "Sentence 2 has 9 tokens\n",
      "Sentence 3 has 14 tokens\n",
      "Sentence 4 has 11 tokens\n",
      "Sentence 5 has 19 tokens\n",
      "Sentence 6 has 14 tokens\n",
      "Sentence 7 has 16 tokens\n",
      "Sentence 8 has 17 tokens\n",
      "Sentence 9 has 12 tokens\n",
      "Sentence 10 has 7 tokens\n",
      "Sentence 11 has 10 tokens\n",
      "Sentence 12 has 13 tokens\n",
      "\n",
      "INFO: File ../data/Dreams/vickie3.txt has 9 sentences\n",
      "Sentence 1 has 14 tokens\n",
      "Sentence 2 has 9 tokens\n",
      "Sentence 3 has 27 tokens\n",
      "Sentence 4 has 12 tokens\n",
      "Sentence 5 has 8 tokens\n",
      "Sentence 6 has 4 tokens\n",
      "Sentence 7 has 4 tokens\n",
      "Sentence 8 has 4 tokens\n",
      "Sentence 9 has 20 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "### Loop 1 ####\n",
    "# Loop1: iterate over all the files in the dreams directory\n",
    "for filename in glob.glob(\"../data/Dreams/*.txt\"): \n",
    "    # read in the file and assign the content to a variable\n",
    "    with open(filename, \"r\") as infile:\n",
    "        content = infile.read()\n",
    "    sentences = nltk.sent_tokenize(content)                            # split the content into sentences\n",
    "    print(f\"INFO: File {filename} has {len(sentences)} sentences\")     # Print the number of sentences in the file\n",
    "\n",
    "    # For each file, assign a number to each sentence. Start with 0:\n",
    "    counter=0\n",
    "\n",
    "    #### Loop 2 ####\n",
    "    # Loop 2: loop over all the sentences in a file:\n",
    "    for sentence in sentences:\n",
    "        counter+=1                                                    # add 1 to the counter\n",
    "        tokens=nltk.word_tokenize(sentence)                           # tokenize the sentence\n",
    "        print(\"Sentence %d has %d tokens\" % (counter, len(tokens)))   # print the number of tokens per sentence\n",
    "               \n",
    "    # print an empty line after each file (this belongs to loop 1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use what we have learned above to write a small NLP program. We will go through all the steps and show how they can be put together. In the last chapters, we have already learned how to write functions. We will make use of this skill here. \n",
    "\n",
    "Our goal is to collect all the nouns from Vickie's dream reports. \n",
    "\n",
    "Before we write actual code, it is always good to consider which steps we need to carry out to reach the goal. \n",
    "\n",
    "Important steps to remember:\n",
    "\n",
    "* create a list of all the files we want to process\n",
    "* open and read the files\n",
    "* tokenize the texts\n",
    "* perform pos-tagging\n",
    "* collect all the tokens analyzed as nouns\n",
    "\n",
    "Remember, we first needed to import `nltk` to use it. \n",
    "\n",
    "Since we want to carry out the same task for each of the files, it is very useful (and good practice!) to write a single function which can do the processing. The following function reads the specified file and returns the tokens with their POS tags:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Writing a processing function for a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tag_tokens_file(filepath):\n",
    "    \"\"\"Read the contents of the file found at the location specified in \n",
    "    FILEPATH and return a list of its tokens with their POS tags.\"\"\"\n",
    "    with open(filepath, \"r\") as infile:\n",
    "        content = infile.read()\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of having to open a file, read the contents and close the file, we can just call the function `tag_tokens_file` to do this. We can test it on a single file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('mom', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('were', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('grocery', 'NN'), ('store', 'NN'), ('.', '.'), ('I', 'PRP'), ('went', 'VBD'), ('over', 'IN'), ('to', 'TO'), ('the', 'DT'), ('free', 'JJ'), ('cookie', 'NN'), ('area', 'NN'), ('.', '.'), ('And', 'CC'), ('this', 'DT'), ('guy', 'NN'), ('gave', 'VBD'), ('me', 'PRP'), ('a', 'DT'), ('cookie', 'NN'), ('.', '.'), ('I', 'PRP'), ('had', 'VBD'), ('seen', 'VBN'), ('the', 'DT'), ('cookies', 'NNS'), (',', ','), ('and', 'CC'), ('they', 'PRP'), ('were', 'VBD'), ('pretend', 'JJ'), ('grasshoppers', 'NNS'), ('.', '.'), ('I', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('spider', 'NN'), ('go', 'VB'), ('by', 'IN'), ('(', '('), ('on', 'IN'), ('the', 'DT'), ('cookie', 'NN'), (')', ')'), ('.', '.'), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Oh', 'UH'), (',', ','), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('like', 'VB'), ('this', 'DT'), ('cookie', 'NN'), ('.', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('thought', 'VBD'), ('it', 'PRP'), ('was', 'VBD'), ('gross', 'JJ'), ('.', '.'), ('But', 'CC'), ('then', 'RB'), ('all', 'PDT'), ('these', 'DT'), ('other', 'JJ'), ('kids', 'NNS'), ('came', 'VBD'), ('.', '.'), ('I', 'PRP'), ('took', 'VBD'), ('the', 'DT'), ('cookie', 'NN'), ('.', '.'), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('what', 'WP'), ('happened', 'VBD'), ('after', 'IN'), ('that', 'DT'), ('about', 'IN'), ('the', 'DT'), ('cookie', 'NN'), ('.', '.'), ('The', 'DT'), ('guy', 'NN'), ('behind', 'IN'), ('the', 'DT'), ('counter', 'NN'), ('gave', 'VBD'), ('us', 'PRP'), ('dolls', 'NNS'), ('.', '.'), ('We', 'PRP'), ('did', 'VBD'), ('something', 'NN'), ('with', 'IN'), ('the', 'DT'), ('dolls', 'NNS'), (',', ','), ('and', 'CC'), ('then', 'RB'), ('we', 'PRP'), ('put', 'VBD'), ('them', 'PRP'), ('back', 'RP'), ('.', '.'), ('I', 'PRP'), ('walked', 'VBD'), ('away', 'RB'), ('to', 'TO'), ('where', 'WRB'), ('my', 'PRP$'), ('mom', 'NN'), ('was', 'VBD'), ('.', '.'), ('This', 'DT'), ('lady', 'NN'), ('came', 'VBD'), ('up', 'RB'), ('to', 'TO'), ('me', 'PRP'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Do', 'VBP'), ('you', 'PRP'), ('want', 'VB'), ('a', 'DT'), ('ride', 'NN'), ('someplace', 'NN'), ('in', 'IN'), ('the', 'DT'), ('university', 'NN'), ('?', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('No', 'UH'), (',', ','), ('we', 'PRP'), ('need', 'VBP'), ('a', 'DT'), ('ride', 'NN'), ('to', 'TO'), ('family', 'NN'), ('housing', 'NN'), ('.', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('remember', 'VBP'), ('her', 'PRP$'), ('in', 'IN'), ('another', 'DT'), ('dream', 'NN'), ('in', 'IN'), ('a', 'DT'), ('grocery', 'NN'), ('store', 'NN'), ('asking', 'VBG'), ('that', 'IN'), ('.', '.'), ('I', 'PRP'), ('just', 'RB'), ('walked', 'VBD'), ('off', 'RP'), ('with', 'IN'), ('my', 'PRP$'), ('mom', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "filename = \"../data/Dreams/vickie1.txt\"\n",
    "tagged_tokens = tag_tokens_file(filename)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Iterating over all the files and applying the processing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this for each of the files in the `../Data/dreams` directory by using a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Dreams/vickie8.txt \n",
      " [('I', 'PRP'), ('had', 'VBD'), ('this', 'DT'), ('horse', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('was', 'VBD'), ('going', 'VBG'), ('to', 'TO'), ('go', 'VB'), ('downtown', 'RB'), ('.', '.'), ('My', 'NNP'), ('friend', 'NN'), (',', ','), ('Sally', 'NNP'), (',', ','), ('was', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('back', 'NN'), ('of', 'IN'), ('me', 'PRP'), ('on', 'IN'), ('the', 'DT'), ('horse', 'NN'), ('.', '.'), ('I', 'PRP'), ('dropped', 'VBD'), ('her', 'PRP'), ('off', 'RP'), ('at', 'IN'), ('the', 'DT'), ('stop', 'NN'), ('sign', 'NN'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('trying', 'VBG'), ('to', 'TO'), ('figure', 'VB'), ('out', 'RP'), ('how', 'WRB'), ('to', 'TO'), ('go', 'VB'), ('downtown', 'JJ'), ('.', '.'), ('I', 'PRP'), ('finally', 'RB'), ('got', 'VBD'), ('there', 'RB'), ('on', 'IN'), ('the', 'DT'), ('horse', 'NN'), ('.', '.'), ('I', 'PRP'), ('saw', 'VBD'), ('Valerie', 'NNP'), ('hanging', 'VBG'), ('out', 'RP'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Hop', 'NNP'), ('on', 'IN'), ('.', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('had', 'VBD'), ('a', 'DT'), ('stereo', 'NN'), ('with', 'IN'), ('music', 'NN'), ('playing', 'NN'), ('.', '.'), ('Jim', 'NNP'), ('Morrison', 'NNP'), ('(', '('), ('from', 'IN'), ('the', 'DT'), ('Doors', 'NNPS'), (')', ')'), ('was', 'VBD'), ('singing', 'VBG'), ('.', '.'), ('This', 'DT'), ('guy', 'NN'), (',', ','), ('who', 'WP'), ('was', 'VBD'), ('in', 'IN'), ('his', 'PRP$'), ('fifties', 'NNS'), (',', ','), ('said', 'VBD'), (',', ','), ('``', '``'), ('Can', 'NN'), ('I', 'PRP'), ('go', 'VBP'), ('too', 'RB'), ('?', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Okay', 'NNP'), ('.', '.'), (\"''\", \"''\"), ('We', 'PRP'), ('rode', 'VBD'), ('home', 'NN'), (',', ','), ('and', 'CC'), ('when', 'WRB'), ('we', 'PRP'), ('went', 'VBD'), ('into', 'IN'), ('my', 'PRP$'), ('bedroom', 'NN'), (',', ','), ('he', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('Jim', 'NNP'), ('Morrison', 'NNP'), ('picture', 'NN'), ('.', '.'), ('He', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('I', 'PRP'), (\"'m\", 'VBP'), ('Jim', 'NNP'), ('Morrison', 'NNP'), ('.', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('was', 'VBD'), ('kind', 'NN'), ('of', 'IN'), ('excited', 'JJ'), ('because', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('Jim', 'NNP'), ('Morrison', 'NNP'), ('.', '.'), ('Mom', 'NNP'), ('was', 'VBD'), ('excited', 'VBN'), ('too', 'RB'), ('.', '.'), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Look', 'VB'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('Jim', 'NNP'), ('Morrison', 'NNP'), ('.', '.'), (\"''\", \"''\"), ('He', 'PRP'), ('even', 'RB'), ('sang', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('song', 'NN'), ('for', 'IN'), ('us', 'PRP'), ('.', '.'), ('Valerie', 'NNP'), ('was', 'VBD'), ('there', 'RB'), ('also', 'RB'), ('.', '.')] \n",
      "\n",
      "../data/Dreams/vickie9.txt \n",
      " [('There', 'EX'), ('was', 'VBD'), ('a', 'DT'), ('beautiful', 'JJ'), ('white', 'JJ'), ('castle', 'NN'), ('with', 'IN'), ('roses', 'NNS'), ('around', 'IN'), ('it', 'PRP'), ('.', '.'), ('There', 'EX'), ('was', 'VBD'), ('a', 'DT'), ('white', 'JJ'), (',', ','), ('gleaming', 'VBG'), ('gate', 'NN'), ('with', 'IN'), ('glitter', 'NN'), ('on', 'IN'), ('it', 'PRP'), ('.', '.'), ('Inside', 'IN'), ('the', 'DT'), ('castle', 'NN'), ('the', 'DT'), ('queen', 'NN'), ('was', 'VBD'), ('beautiful', 'JJ'), ('.', '.'), ('She', 'PRP'), ('had', 'VBD'), ('a', 'DT'), ('diamond', 'NN'), ('tiara', 'NN'), ('on', 'IN'), ('her', 'PRP$'), ('head', 'NN'), ('.', '.'), ('She', 'PRP'), ('had', 'VBD'), ('silk', 'NN'), ('around', 'IN'), ('her', 'PRP$'), ('neck', 'NN'), ('and', 'CC'), ('a', 'DT'), ('long', 'JJ'), (',', ','), ('full', 'JJ'), ('blue', 'NN'), ('velvet', 'NN'), ('dress', 'NN'), ('with', 'IN'), ('purple', 'JJ'), ('lace', 'NN'), ('around', 'IN'), ('it', 'PRP'), ('.', '.'), ('She', 'PRP'), ('had', 'VBD'), ('a', 'DT'), ('diamond', 'NN'), ('necklace', 'NN'), ('.', '.'), ('She', 'PRP'), ('had', 'VBD'), ('ladies', 'NNS'), ('in', 'IN'), ('waiting', 'VBG'), ('.', '.'), ('They', 'PRP'), ('were', 'VBD'), ('kind', 'NN'), ('of', 'IN'), ('dressed', 'VBN'), ('the', 'DT'), ('same', 'JJ'), (',', ','), ('but', 'CC'), ('not', 'RB'), ('as', 'IN'), ('richly', 'RB'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('just', 'RB'), ('watching', 'VBG'), ('this', 'DT'), (',', ','), ('but', 'CC'), ('they', 'PRP'), ('seemed', 'VBD'), ('to', 'TO'), ('be', 'VB'), ('aware', 'JJ'), ('of', 'IN'), ('my', 'PRP$'), ('presence', 'NN'), ('.', '.')] \n",
      "\n",
      "../data/Dreams/vickie10.txt \n",
      " [('I', 'PRP'), ('am', 'VBP'), ('at', 'IN'), ('the', 'DT'), ('bus', 'NN'), ('stop', 'VB'), ('waiting', 'VBG'), ('to', 'TO'), ('go', 'VB'), ('to', 'TO'), ('Bonnie', 'NNP'), (\"'s\", 'POS'), ('(', '('), ('7-year-old', 'JJ'), ('friend', 'NN'), (')', ')'), ('party', 'NN'), ('.', '.'), ('I', 'PRP'), ('feel', 'VBP'), ('really', 'RB'), ('confused', 'VBN'), ('.', '.'), ('Then', 'RB'), ('I', 'PRP'), ('go', 'VBP'), ('and', 'CC'), ('I', 'PRP'), (\"'m\", 'VBP'), ('in', 'IN'), ('this', 'DT'), ('water', 'NN'), (';', ':'), ('a', 'DT'), ('very', 'RB'), ('small', 'JJ'), ('pool', 'NN'), ('.', '.'), ('I', 'PRP'), ('saw', 'VBD'), ('Charlotte', 'NNP'), ('(', '('), ('11-year-old', 'JJ'), ('friend', 'NN'), (')', ')'), ('with', 'IN'), ('her', 'PRP$'), ('mother', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('say', 'VBP'), (',', ','), ('``', '``'), ('Hi', 'NNP'), ('.', '.'), (\"''\", \"''\"), ('Then', 'RB'), ('the', 'DT'), ('dream', 'NN'), ('goes', 'VBZ'), ('to', 'TO'), ('this', 'DT'), ('guy', 'NN'), ('in', 'IN'), ('this', 'DT'), ('office', 'NN'), ('.', '.'), ('He', 'PRP'), ('has', 'VBZ'), ('a', 'DT'), ('computer', 'NN'), ('and', 'CC'), ('there', 'EX'), ('are', 'VBP'), ('rectangle', 'JJ'), ('things', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('computer', 'NN'), ('.', '.'), ('He', 'PRP'), ('put', 'VBD'), ('his', 'PRP$'), ('hand', 'NN'), ('on', 'IN'), ('the', 'DT'), ('computer', 'NN'), ('and', 'CC'), ('they', 'PRP'), ('(', '('), ('the', 'DT'), ('rectangle', 'NN'), ('things', 'NNS'), (')', ')'), ('stuck', 'VBP'), ('to', 'TO'), ('him', 'PRP'), ('.', '.'), ('So', 'RB'), ('he', 'PRP'), ('put', 'VBD'), ('them', 'PRP'), ('somewhere', 'RB'), ('to', 'TO'), ('hide', 'VB'), ('them', 'PRP'), ('.', '.'), ('His', 'PRP$'), ('boss', 'NN'), ('comes', 'VBZ'), ('in', 'IN'), ('.', '.'), ('He', 'PRP'), ('thinks', 'VBZ'), ('the', 'DT'), ('boss', 'NN'), ('saw', 'VBD'), ('him', 'PRP'), (',', ','), ('and', 'CC'), ('he', 'PRP'), (\"'s\", 'VBZ'), ('all', 'DT'), ('nervous', 'JJ'), ('.', '.'), ('The', 'DT'), ('boss', 'NN'), ('was', 'VBD'), ('mad', 'JJ'), (',', ','), ('but', 'CC'), ('did', 'VBD'), (\"n't\", 'RB'), ('see', 'VB'), ('.', '.'), ('Then', 'RB'), ('the', 'DT'), ('boss', 'NN'), ('saw', 'VBD'), ('this', 'DT'), ('suitcase', 'NN'), ('with', 'IN'), ('these', 'DT'), ('rectangle', 'JJ'), ('things', 'NNS'), ('in', 'IN'), ('it', 'PRP'), ('.', '.'), ('The', 'DT'), ('boss', 'NN'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Why', 'WRB'), ('are', 'VBP'), ('these', 'DT'), ('there', 'RB'), ('?', '.'), ('You', 'PRP'), (\"'re\", 'VBP'), ('a', 'DT'), ('sick', 'JJ'), ('man', 'NN'), ('.', '.'), (\"''\", \"''\"), ('They', 'PRP'), ('were', 'VBD'), ('really', 'RB'), ('Charlotte', 'NNP'), (\"'s\", 'POS'), ('clothes', 'NNS'), ('that', 'IN'), ('she', 'PRP'), ('was', 'VBD'), ('bringing', 'VBG'), ('to', 'TO'), ('my', 'PRP$'), ('house', 'NN'), (',', ','), ('and', 'CC'), ('that', 'DT'), ('was', 'VBD'), ('her', 'PRP$'), ('father', 'NN'), ('(', '('), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('rectangle', 'JJ'), ('thing', 'NN'), (')', ')'), ('.', '.'), ('She', 'PRP'), ('forgot', 'VBD'), ('the', 'DT'), ('suitcase', 'NN'), ('at', 'IN'), ('his', 'PRP$'), ('office', 'NN'), ('.', '.')] \n",
      "\n",
      "../data/Dreams/vickie7.txt \n",
      " [('Valerie', 'NNP'), (',', ','), ('Nancy', 'NNP'), ('(', '('), ('19-year-old', 'JJ'), ('sister', 'NN'), (')', ')'), ('and', 'CC'), ('I', 'PRP'), ('were', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('house', 'NN'), ('alone', 'RB'), ('.', '.'), ('Nancy', 'NNP'), ('went', 'VBD'), ('outside', 'IN'), ('to', 'TO'), ('smoke', 'VB'), ('a', 'DT'), ('cigarette', 'NN'), ('.', '.'), ('This', 'DT'), ('man', 'NN'), ('and', 'CC'), ('this', 'DT'), ('lady', 'NN'), ('came', 'VBD'), ('up', 'RB'), ('to', 'TO'), ('her', 'PRP$'), ('and', 'CC'), ('grabbed', 'VBD'), ('her', 'PRP$'), ('arms', 'NNS'), ('.', '.'), ('They', 'PRP'), ('held', 'VBD'), ('a', 'DT'), ('gun', 'NN'), ('to', 'TO'), ('her', 'PRP$'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Where', 'WRB'), (\"'s\", 'VBZ'), ('your', 'PRP$'), ('mom', 'NN'), (\"'s\", 'POS'), ('money', 'NN'), ('?', '.'), (\"''\", \"''\"), ('We', 'PRP'), ('told', 'VBD'), ('them', 'PRP'), ('we', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('have', 'VB'), ('any', 'DT'), ('money', 'NN'), ('.', '.'), ('They', 'PRP'), ('took', 'VBD'), ('out', 'RP'), ('a', 'DT'), ('gun', 'NN'), ('.', '.'), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Fine', 'NNP'), (',', ','), ('we', 'PRP'), ('have', 'VBP'), ('lots', 'NNS'), ('of', 'IN'), ('money', 'NN'), ('.', '.'), (\"''\", \"''\"), ('Then', 'RB'), ('Nancy', 'NNP'), ('came', 'VBD'), ('up', 'RB'), ('to', 'TO'), ('me', 'PRP'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('We', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('have', 'VB'), ('that', 'DT'), ('much', 'JJ'), ('money', 'NN'), ('.', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('I', 'PRP'), ('know', 'VBP'), ('.', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('tricking', 'VBG'), ('them', 'PRP'), ('.', '.'), (\"''\", \"''\"), ('They', 'PRP'), ('said', 'VBD'), ('they', 'PRP'), ('would', 'MD'), ('come', 'VB'), ('back', 'RB'), ('.', '.'), ('Nancy', 'NN'), ('left', 'VBD'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('only', 'JJ'), ('one', 'CD'), ('there', 'RB'), ('.', '.'), ('I', 'PRP'), ('locked', 'VBD'), ('all', 'PDT'), ('the', 'DT'), ('doors', 'NNS'), ('.', '.'), ('I', 'PRP'), ('ran', 'VBD'), ('and', 'CC'), ('I', 'PRP'), ('called', 'VBD'), ('for', 'IN'), ('help', 'NN'), ('because', 'IN'), ('I', 'PRP'), ('knew', 'VBD'), ('they', 'PRP'), ('were', 'VBD'), ('going', 'VBG'), ('to', 'TO'), ('come', 'VB'), ('soon', 'RB'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('scared', 'VBN'), ('.', '.'), ('I', 'PRP'), ('thought', 'VBD'), ('they', 'PRP'), ('might', 'MD'), ('kill', 'VB'), ('Nancy', 'NNP'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('also', 'RB'), ('scared', 'VBN'), ('that', 'IN'), ('we', 'PRP'), ('might', 'MD'), ('lose', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('.', '.'), ('I', 'PRP'), ('saw', 'VBD'), ('this', 'DT'), ('guy', 'NN'), (',', ','), ('Mark', 'NNP'), (';', ':'), ('he', 'PRP'), ('was', 'VBD'), ('from', 'IN'), ('my', 'PRP$'), ('school', 'NN'), ('.', '.'), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Help', 'VB'), ('me', 'PRP'), ('!', '.'), (\"''\", \"''\"), ('There', 'EX'), ('was', 'VBD'), ('this', 'DT'), ('other', 'JJ'), ('man', 'NN'), ('.', '.'), ('Nancy', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('were', 'VBD'), ('running', 'VBG'), ('outside', 'JJ'), ('and', 'CC'), ('trying', 'VBG'), ('not', 'RB'), ('to', 'TO'), ('be', 'VB'), ('seen', 'VBN'), ('.', '.'), ('We', 'PRP'), ('ran', 'VBD'), ('up', 'RB'), ('to', 'TO'), ('this', 'DT'), ('place', 'NN'), ('that', 'WDT'), ('was', 'VBD'), ('a', 'DT'), ('bar', 'NN'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), ('drink', 'NN'), ('of', 'IN'), ('soda', 'NN'), ('.', '.')] \n",
      "\n",
      "../data/Dreams/vickie6.txt \n",
      " [('We', 'PRP'), ('got', 'VBD'), ('this', 'DT'), ('new', 'JJ'), ('house', 'NN'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('really', 'RB'), (',', ','), ('really', 'RB'), ('big', 'JJ'), ('.', '.'), ('Allison', 'NNP'), ('(', '('), ('11-year-old', 'JJ'), ('former', 'JJ'), ('neighbor', 'NN'), (',', ','), ('Jeb', 'NNP'), ('(', '('), ('8-year-old', 'JJ'), ('neighbor', 'NN'), (')', ')'), (',', ','), ('and', 'CC'), ('Mary', 'NNP'), ('(', '('), ('5-year-old', 'JJ'), ('sister', 'NN'), ('of', 'IN'), ('Jeb', 'NNP'), (')', ')'), ('came', 'VBD'), ('over', 'IN'), ('.', '.'), ('I', 'PRP'), ('went', 'VBD'), ('in', 'IN'), ('this', 'DT'), ('room', 'NN'), (',', ','), ('and', 'CC'), ('they', 'PRP'), ('tried', 'VBD'), ('to', 'TO'), ('trap', 'VB'), ('me', 'PRP'), ('.', '.'), ('But', 'CC'), ('I', 'PRP'), ('got', 'VBD'), ('out', 'RB'), ('and', 'CC'), ('that', 'DT'), ('was', 'VBD'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('them', 'PRP'), ('.', '.'), ('I', 'PRP'), ('felt', 'VBD'), ('like', 'IN'), ('Ð', 'NN'), ('just', 'RB'), ('go', 'VB'), ('away', 'RB'), (';', ':'), ('this', 'DT'), ('is', 'VBZ'), ('my', 'PRP$'), ('house', 'NN'), ('.', '.'), ('We', 'PRP'), ('were', 'VBD'), ('looking', 'VBG'), ('through', 'IN'), ('all', 'PDT'), ('these', 'DT'), ('really', 'RB'), ('nice', 'JJ'), ('rooms', 'NNS'), ('.', '.'), ('A', 'DT'), ('hospital', 'NN'), ('was', 'VBD'), ('on', 'IN'), ('top', 'NN'), ('of', 'IN'), ('us', 'PRP'), ('.', '.'), ('We', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('store', 'NN'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('one', 'CD'), ('of', 'IN'), ('those', 'DT'), ('really', 'RB'), (',', ','), ('really', 'RB'), ('scary', 'JJ'), ('stores', 'NNS'), ('that', 'WDT'), ('was', 'VBD'), ('supposed', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('scary', 'JJ'), ('.', '.'), ('The', 'DT'), ('store', 'NN'), ('was', 'VBD'), ('in', 'IN'), ('our', 'PRP$'), ('house', 'NN'), ('.', '.'), ('In', 'IN'), ('the', 'DT'), ('dream', 'NN'), ('I', 'PRP'), ('was', 'VBD'), ('thinking', 'VBG'), (',', ','), ('``', '``'), ('Hey', 'NNP'), (',', ','), ('I', 'PRP'), (\"'ve\", 'VBP'), ('seen', 'VBN'), ('that', 'IN'), ('in', 'IN'), ('another', 'DT'), ('dream', 'NN'), ('.', '.'), (\"''\", \"''\"), ('And', 'CC'), ('I', 'PRP'), ('have', 'VBP'), ('.', '.'), ('Doug', 'NNP'), ('(', '('), ('sister', 'JJ'), ('Valerie', 'NNP'), (\"'s\", 'POS'), ('boyfriend', 'NN'), (')', ')'), ('was', 'VBD'), ('there', 'RB'), ('.', '.'), ('He', 'PRP'), ('went', 'VBD'), ('with', 'IN'), ('us', 'PRP'), ('.', '.'), ('He', 'PRP'), ('went', 'VBD'), ('off', 'RP'), ('into', 'IN'), ('his', 'PRP$'), ('truck', 'NN'), ('and', 'CC'), ('just', 'RB'), ('drove', 'VB'), ('away', 'RB'), ('.', '.'), ('And', 'CC'), ('there', 'EX'), (\"'s\", 'VBZ'), ('supposed', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('a', 'DT'), ('ghost', 'NN'), ('in', 'IN'), ('there', 'RB'), ('.', '.')] \n",
      "\n",
      "../data/Dreams/vickie4.txt \n",
      " [('I', 'PRP'), ('knocked', 'VBD'), ('on', 'IN'), ('Sally', 'NNP'), (\"'s\", 'POS'), ('door', 'NN'), ('(', '('), ('9-year-old', 'CD'), ('friend', 'NN'), (')', ')'), ('.', '.'), ('Her', 'PRP$'), ('dad', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('they', 'PRP'), ('were', 'VBD'), ('going', 'VBG'), ('out', 'RB'), ('someplace', 'NN'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('I', 'PRP'), ('could', 'MD'), ('come', 'VB'), ('with', 'IN'), ('them', 'PRP'), ('.', '.'), ('We', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('this', 'DT'), ('place', 'NN'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('fun', 'JJ'), ('place', 'NN'), ('that', 'IN'), ('these', 'DT'), ('two', 'CD'), ('girls', 'NNS'), ('had', 'VBD'), ('made', 'VBN'), ('up', 'RP'), ('.', '.'), ('The', 'DT'), ('first', 'JJ'), ('thing', 'NN'), ('we', 'PRP'), ('did', 'VBD'), ('was', 'VBD'), ('go', 'VB'), ('down', 'RP'), ('the', 'DT'), ('slide', 'NN'), ('.', '.'), ('There', 'EX'), (\"'s\", 'VBZ'), ('a', 'DT'), ('wire', 'NN'), ('in', 'IN'), ('the', 'DT'), ('middle', 'NN'), ('and', 'CC'), ('you', 'PRP'), ('spring', 'VBP'), ('up', 'RP'), ('in', 'IN'), ('the', 'DT'), ('air', 'NN'), ('.', '.'), ('You', 'PRP'), ('hold', 'VBP'), ('onto', 'IN'), ('the', 'DT'), ('wire', 'NN'), (',', ','), ('and', 'CC'), ('you', 'PRP'), ('go', 'VBP'), ('back', 'RB'), ('onto', 'IN'), ('the', 'DT'), ('slide', 'NN'), (',', ','), ('and', 'CC'), ('you', 'PRP'), ('slide', 'VBP'), ('down', 'RB'), ('.', '.'), ('The', 'DT'), ('slide', 'NN'), ('was', 'VBD'), ('really', 'RB'), ('fun', 'JJ'), ('and', 'CC'), ('exciting', 'VBG'), ('.', '.'), ('Then', 'RB'), ('there', 'EX'), (\"'s\", 'VBZ'), ('this', 'DT'), ('little', 'JJ'), ('house', 'NN'), ('thing', 'NN'), ('.', '.'), ('It', 'PRP'), ('had', 'VBD'), ('all', 'PDT'), ('these', 'DT'), ('nice', 'JJ'), ('clothes', 'NNS'), (':', ':'), ('cowboy', 'NN'), ('hats', 'NNS'), (',', ','), ('fur', 'NN'), (',', ','), ('sparkly', 'JJ'), ('things', 'NNS'), (',', ','), ('everything', 'NN'), ('you', 'PRP'), ('could', 'MD'), ('imagine', 'VB'), ('that', 'IN'), ('you', 'PRP'), ('could', 'MD'), ('dress', 'VB'), ('up', 'RP'), ('in', 'IN'), ('.', '.'), ('I', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('find', 'VB'), ('all', 'DT'), ('of', 'IN'), ('it', 'PRP'), ('.', '.'), ('The', 'DT'), ('dream', 'NN'), ('ended', 'VBD'), ('when', 'WRB'), ('I', 'PRP'), ('was', 'VBD'), ('trying', 'VBG'), ('to', 'TO'), ('find', 'VB'), ('something', 'NN'), ('to', 'TO'), ('wear', 'VB'), ('.', '.')] \n",
      "\n",
      "../data/Dreams/vickie5.txt \n",
      " [('I', 'PRP'), ('think', 'VBP'), ('my', 'PRP$'), ('dream', 'NN'), ('had', 'VBD'), ('my', 'PRP$'), ('mom', 'NN'), ('and', 'CC'), ('my', 'PRP$'), ('sister', 'NN'), ('Valerie', 'NNP'), ('in', 'IN'), ('it', 'PRP'), ('.', '.'), ('But', 'CC'), ('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('really', 'RB'), ('see', 'VB'), ('her', 'PRP'), ('.', '.'), ('This', 'DT'), ('guy', 'NN'), ('from', 'IN'), ('downtown', 'NN'), ('was', 'VBD'), ('trying', 'VBG'), ('to', 'TO'), ('break', 'VB'), ('in', 'IN'), ('.', '.'), ('He', 'PRP'), ('finally', 'RB'), ('opened', 'VBD'), ('the', 'DT'), ('lock', 'NN'), ('.', '.'), ('I', 'PRP'), ('ran', 'VBP'), ('after', 'IN'), ('him', 'PRP'), ('because', 'IN'), ('he', 'PRP'), ('was', 'VBD'), ('going', 'VBG'), ('upstairs', 'RB'), ('into', 'IN'), ('my', 'PRP$'), ('mom', 'NN'), (\"'s\", 'POS'), ('room', 'NN'), ('.', '.'), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('How', 'WRB'), ('would', 'MD'), ('you', 'PRP'), ('like', 'IN'), ('not', 'RB'), ('to', 'TO'), ('be', 'VB'), ('Valerie', 'NNP'), (\"'s\", 'POS'), ('friend', 'NN'), ('.', '.'), ('Why', 'WRB'), ('do', 'VBP'), (\"n't\", 'RB'), ('you', 'PRP'), ('just', 'RB'), ('rob', 'VB'), ('yourself', 'PRP'), ('?', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Why', 'WRB'), ('do', 'VBP'), (\"n't\", 'RB'), ('you', 'PRP'), ('rob', 'VB'), ('your', 'PRP$'), ('mother', 'NN'), ('?', '.'), (\"''\", \"''\"), ('He', 'PRP'), ('said', 'VBD'), ('he', 'PRP'), ('lives', 'VBZ'), ('with', 'IN'), ('this', 'DT'), ('girl', 'NN'), ('named', 'VBN'), ('Bess', 'NNP'), ('.', '.')] \n",
      "\n",
      "../data/Dreams/vickie1.txt \n",
      " [('My', 'PRP$'), ('mom', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('were', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('grocery', 'NN'), ('store', 'NN'), ('.', '.'), ('I', 'PRP'), ('went', 'VBD'), ('over', 'IN'), ('to', 'TO'), ('the', 'DT'), ('free', 'JJ'), ('cookie', 'NN'), ('area', 'NN'), ('.', '.'), ('And', 'CC'), ('this', 'DT'), ('guy', 'NN'), ('gave', 'VBD'), ('me', 'PRP'), ('a', 'DT'), ('cookie', 'NN'), ('.', '.'), ('I', 'PRP'), ('had', 'VBD'), ('seen', 'VBN'), ('the', 'DT'), ('cookies', 'NNS'), (',', ','), ('and', 'CC'), ('they', 'PRP'), ('were', 'VBD'), ('pretend', 'JJ'), ('grasshoppers', 'NNS'), ('.', '.'), ('I', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('spider', 'NN'), ('go', 'VB'), ('by', 'IN'), ('(', '('), ('on', 'IN'), ('the', 'DT'), ('cookie', 'NN'), (')', ')'), ('.', '.'), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Oh', 'UH'), (',', ','), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('like', 'VB'), ('this', 'DT'), ('cookie', 'NN'), ('.', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('thought', 'VBD'), ('it', 'PRP'), ('was', 'VBD'), ('gross', 'JJ'), ('.', '.'), ('But', 'CC'), ('then', 'RB'), ('all', 'PDT'), ('these', 'DT'), ('other', 'JJ'), ('kids', 'NNS'), ('came', 'VBD'), ('.', '.'), ('I', 'PRP'), ('took', 'VBD'), ('the', 'DT'), ('cookie', 'NN'), ('.', '.'), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('what', 'WP'), ('happened', 'VBD'), ('after', 'IN'), ('that', 'DT'), ('about', 'IN'), ('the', 'DT'), ('cookie', 'NN'), ('.', '.'), ('The', 'DT'), ('guy', 'NN'), ('behind', 'IN'), ('the', 'DT'), ('counter', 'NN'), ('gave', 'VBD'), ('us', 'PRP'), ('dolls', 'NNS'), ('.', '.'), ('We', 'PRP'), ('did', 'VBD'), ('something', 'NN'), ('with', 'IN'), ('the', 'DT'), ('dolls', 'NNS'), (',', ','), ('and', 'CC'), ('then', 'RB'), ('we', 'PRP'), ('put', 'VBD'), ('them', 'PRP'), ('back', 'RP'), ('.', '.'), ('I', 'PRP'), ('walked', 'VBD'), ('away', 'RB'), ('to', 'TO'), ('where', 'WRB'), ('my', 'PRP$'), ('mom', 'NN'), ('was', 'VBD'), ('.', '.'), ('This', 'DT'), ('lady', 'NN'), ('came', 'VBD'), ('up', 'RB'), ('to', 'TO'), ('me', 'PRP'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Do', 'VBP'), ('you', 'PRP'), ('want', 'VB'), ('a', 'DT'), ('ride', 'NN'), ('someplace', 'NN'), ('in', 'IN'), ('the', 'DT'), ('university', 'NN'), ('?', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('No', 'UH'), (',', ','), ('we', 'PRP'), ('need', 'VBP'), ('a', 'DT'), ('ride', 'NN'), ('to', 'TO'), ('family', 'NN'), ('housing', 'NN'), ('.', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('remember', 'VBP'), ('her', 'PRP$'), ('in', 'IN'), ('another', 'DT'), ('dream', 'NN'), ('in', 'IN'), ('a', 'DT'), ('grocery', 'NN'), ('store', 'NN'), ('asking', 'VBG'), ('that', 'IN'), ('.', '.'), ('I', 'PRP'), ('just', 'RB'), ('walked', 'VBD'), ('off', 'RP'), ('with', 'IN'), ('my', 'PRP$'), ('mom', 'NN'), ('.', '.')] \n",
      "\n",
      "../data/Dreams/vickie2.txt \n",
      " [('This', 'DT'), ('was', 'VBD'), ('a', 'DT'), ('place', 'NN'), ('where', 'WRB'), ('you', 'PRP'), ('can', 'MD'), ('play', 'VB'), ('with', 'IN'), ('all', 'PDT'), ('these', 'DT'), ('toys', 'NNS'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('kind', 'NN'), ('of', 'IN'), ('like', 'IN'), ('Toys', 'NNP'), ('R', 'NNP'), ('Us', 'NNP'), ('.', '.'), ('I', 'PRP'), ('lost', 'VBD'), ('a', 'DT'), ('ball', 'NN'), ('I', 'PRP'), ('just', 'RB'), ('got', 'VBD'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('was', 'VBD'), ('really', 'RB'), ('frantic', 'JJ'), ('.', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('walking', 'VBG'), ('around', 'RB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('see', 'VBP'), ('other', 'JJ'), ('balls', 'NNS'), ('.', '.'), ('But', 'CC'), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('see', 'VB'), ('the', 'DT'), ('ball', 'NN'), ('I', 'PRP'), ('lost', 'VBD'), ('that', 'IN'), ('my', 'PRP$'), ('mom', 'NN'), ('just', 'RB'), ('bought', 'VBD'), ('me', 'PRP'), ('at', 'IN'), ('the', 'DT'), ('store', 'NN'), ('.', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('playing', 'VBG'), ('with', 'IN'), ('all', 'PDT'), ('these', 'DT'), ('toys', 'VBZ'), ('a', 'DT'), ('little', 'JJ'), ('as', 'IN'), ('I', 'PRP'), ('go', 'VBP'), ('on', 'IN'), ('.', '.'), ('Finally', 'NNP'), (',', ','), ('I', 'PRP'), ('just', 'RB'), ('stop', 'VB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('sit', 'VBP'), ('there', 'EX'), ('and', 'CC'), ('look', 'NN'), ('at', 'IN'), ('these', 'DT'), ('toys', 'NNS'), ('.', '.'), ('This', 'DT'), ('guy', 'NN'), ('comes', 'VBZ'), ('up', 'RP'), ('to', 'TO'), ('me', 'PRP'), ('and', 'CC'), ('says', 'VBZ'), (',', ','), ('``', '``'), ('Are', 'VBP'), ('you', 'PRP'), ('looking', 'VBG'), ('for', 'IN'), ('something', 'NN'), ('?', '.'), (\"''\", \"''\"), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('I', 'PRP'), (\"'m\", 'VBP'), ('looking', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('ball', 'NN'), ('.', '.'), (\"''\", \"''\"), ('He', 'PRP'), ('took', 'VBD'), ('something', 'NN'), ('from', 'IN'), ('a', 'DT'), ('bag', 'NN'), ('.', '.'), ('He', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Is', 'VBZ'), ('this', 'DT'), ('your', 'PRP$'), ('ball', 'NN'), ('?', '.'), (\"''\", \"''\"), ('And', 'CC'), ('I', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Yeah', 'UH'), (',', ','), ('that', 'DT'), (\"'s\", 'POS'), ('my', 'PRP$'), ('ball', 'NN'), ('.', '.'), (\"''\", \"''\")] \n",
      "\n",
      "../data/Dreams/vickie3.txt \n",
      " [('Wendy', 'NNP'), ('8-year-old', 'JJ'), ('friend', 'NN'), ('and', 'CC'), ('her', 'PRP$'), ('brother', 'NN'), ('(', '('), ('10', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (')', ')'), ('were', 'VBD'), ('packing', 'VBG'), ('.', '.'), ('Wendy', 'NNP'), (\"'s\", 'POS'), ('brother', 'NN'), (\"'s\", 'POS'), ('bed', 'NN'), ('was', 'VBD'), (\"n't\", 'RB'), ('there', 'RB'), ('.', '.'), ('There', 'EX'), ('was', 'VBD'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('junk', 'NN'), ('that', 'WDT'), ('had', 'VBD'), ('been', 'VBN'), ('under', 'IN'), ('it', 'PRP'), ('(', '('), ('which', 'WDT'), ('is', 'VBZ'), ('true', 'JJ'), ('in', 'IN'), ('real', 'JJ'), ('life', 'NN'), (')', ')'), ('except', 'IN'), ('I', 'PRP'), ('found', 'VBD'), ('this', 'DT'), ('picture', 'NN'), ('of', 'IN'), ('me', 'PRP'), ('.', '.'), ('On', 'IN'), ('the', 'DT'), ('back', 'NN'), ('it', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('I', 'PRP'), ('love', 'VBP'), ('Vickie', 'NNP'), ('.', '.'), (\"''\", \"''\"), ('This', 'DT'), ('was', 'VBD'), ('written', 'VBN'), ('by', 'IN'), ('Wendy', 'NNP'), (\"'s\", 'POS'), ('brother', 'NN'), ('.', '.'), ('I', 'PRP'), ('showed', 'VBD'), ('Wendy', 'NNP'), ('.', '.'), ('We', 'PRP'), ('went', 'VBD'), ('outside', 'RB'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('sent', 'VBN'), ('.', '.'), ('I', 'PRP'), ('thought', 'VBD'), (',', ','), ('``', '``'), ('Oh', 'UH'), (',', ','), ('my', 'PRP$'), ('god', 'NN'), (',', ','), (\"''\", \"''\"), ('because', 'IN'), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('really', 'RB'), ('like', 'JJ'), ('Wendy', 'NNP'), (\"'s\", 'POS'), ('brother', 'NN'), ('.', '.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Iterate over the `.txt` files in the directory and perform POS tagging on each of them\n",
    "for filename in glob.glob(\"../data/Dreams/*.txt\"): \n",
    "    tagged_tokens = tag_tokens_file(filename)\n",
    "    print(filename, \"\\n\", tagged_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Collecting all the nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extend this code a bit so that we don't print all POS-tagged tokens of each file, but we get all (proper) nouns from the texts and add them to a list called `nouns_in_dreams`. Then, we print the set of nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hey', 'Toys', 'head', 'kind', 'My', 'Sally', 'necklace', 'drink', 'Morrison', 'computer', 'pool', 'Charlotte', 'back', 'Fine', 'mother', 'university', 'middle', 'friend', 'song', 'Hop', 'house', 'dad', 'diamond', 'Doug', 'silk', 'Jeb', 'thing', 'stereo', 'Mark', 'Jim', 'picture', 'velvet', 'water', 'hand', 'soda', 'boss', 'lady', 'everything', 'school', 'bag', 'counter', 'downtown', 'Allison', 'cookie', 'presence', 'Vickie', 'stop', 'sister', 'sign', 'ball', 'door', 'cowboy', 'family', 'gun', 'Finally', 'office', 'girl', 'Ð', 'glitter', 'dress', 'Bess', 'mom', 'store', 'Us', 'dream', 'top', 'bar', 'life', 'father', 'god', 'castle', 'neck', 'air', 'truck', 'wire', 'help', 'fur', 'look', 'cigarette', 'bus', 'bunch', 'man', 'ghost', 'grocery', 'party', 'room', 'Wendy', 'spider', 'Mom', 'brother', 'horse', 'Nancy', 'Valerie', 'place', 'ride', 'bedroom', 'Bonnie', 'Hi', 'money', 'housing', 'neighbor', 'hospital', 'lock', 'someplace', 'queen', 'rectangle', 'Can', 'lace', 'R', 'Mary', 'gate', 'tiara', 'area', 'suitcase', 'blue', 'slide', 'junk', 'home', 'bed', 'boyfriend', 'guy', 'end', 'Okay', 'playing', 'music', 'something'}\n"
     ]
    }
   ],
   "source": [
    "# Create a list that will contain all nouns\n",
    "nouns_in_dreams = []\n",
    "\n",
    "# Iterate over the `.txt` files in the directory and perform POS tagging on each of them\n",
    "for filename in glob.glob(\"../data/Dreams/*.txt\"): \n",
    "    tagged_tokens = tag_tokens_file(filename)\n",
    "        \n",
    "    # Get all (proper) nouns in the text (\"NN\" and \"NNP\") and add them to the list\n",
    "    for token, pos in tagged_tokens:\n",
    "        if pos in [\"NN\", \"NNP\"]:\n",
    "            nouns_in_dreams.append(token)\n",
    "\n",
    "# Print the set of nouns in all dreams\n",
    "print(set(nouns_in_dreams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an idea what Vickie dreams about!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** \n",
    "\n",
    "Try to collect all the present participles in the the text store in `../data/charlie.txt` using the NLTK tokenizer and POS-tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following list: \n",
    "`['boiling', 'bubbling', 'hissing', 'sizzling', 'clanking', 'running', 'hopping', 'knowing', 'rubbing', 'cackling', 'going']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dd5a79451792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# but if you want to use it, you can probably figure out how it works yourself :-)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# If our code is correct, we should get a compliment :-)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_participles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m11\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_participles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Well done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we can test our code using the assert statement (don't worry about this now, \n",
    "# but if you want to use it, you can probably figure out how it works yourself :-) \n",
    "# If our code is correct, we should get a compliment :-)\n",
    "assert len(present_participles) == 11 and type(present_participles[0]) == str\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** \n",
    "\n",
    "The resulting list `verb_lemmas` above contains a lot of duplicates. Do you remember how you can get rid of these duplicates? Create a set in which each verb occurs only once and name it `unique_verbs`. Then print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## the list is stored under the variable 'verb_lemmas'\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(unique_verbs) == 28    \n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** \n",
    "\n",
    "Now use a for-loop to count the number of times that each of these verb lemmas occurs in the text! For each verb in the list you just created, get the count of this verb in `charlie.txt` using the `count()` method. Create a dictionary that contains the lemmas of the verbs as keys, and the counts of these verbs as values. Refer to the notebook about Topic 1 if you forgot how to use the `count()` method or how to create dictionary entries!\n",
    "\n",
    "Tip: you don't need to read in the file again, you can just use the list called verb_lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "verb_counts = {}\n",
    "\n",
    "# Finish this for-loop\n",
    "for verb in unique_verbs:\n",
    "    # your code here\n",
    "\n",
    "print(verb_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(verb_counts) == 28 and verb_counts[\"bubble\"] == 1 and verb_counts[\"be\"] == 9\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:**\n",
    "    \n",
    "Write your counts to a file called `charlie_verb_counts.txt` and write it to `../data` in the following format:\n",
    "\n",
    "verb, count\n",
    "\n",
    "verb, count \n",
    "\n",
    "...\n",
    "\n",
    "Don't forget to use newline characters at the end of each line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
